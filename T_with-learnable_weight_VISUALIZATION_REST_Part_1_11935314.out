wandb: Currently logged in as: piconda (drs). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/lcur2470/RS/wandb/run-20230623_153547-1bj5xew6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run T-DiffRec_ml-1m_clean_1_Visualization
wandb: ‚≠êÔ∏è View project at https://wandb.ai/drs/drs
wandb: üöÄ View run at https://wandb.ai/drs/drs/runs/1bj5xew6
args: Namespace(batch_size=400, cuda=True, data_path='./datasets/', dataset='ml-1m_clean', dims='[1000]', emb_size=10, epochs=1000, gpu='1', log_name='log', lr=0.0001, mean_type='x0_learnable', model_type='T-DiffRec', noise_max=0.02, noise_min=0.0001, noise_scale=0.1, noise_schedule='linear-var', norm=False, num_workers=4, patience=20, reweight=True, round=1, run_name='Visualization', sampling_noise=False, sampling_steps=0, save_path='./saved_models/', seed=1, steps=5, time_type='cat', topN='[10, 20, 50, 100]', tst_w_val=False, visualize_weights=False, w_max=1.0, w_min=0.1, weight_decay=0.0, workers=10)
Starting time:  2023-06-23 15:35:51
user num: 5949
item num: 2810
data ready.
Embedding size: 10
models ready.
Number of all parameters: 5633970
Start training...
Runing Epoch 001 train loss 6309.8419 costs 00: 00: 01
------------------------------------------------------
Runing Epoch 002 train loss 5588.6492 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 003 train loss 5106.6729 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 004 train loss 4735.5258 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0593
0.0547
0.0449
0.0367
 0.0413
0.0745
0.1427
0.2186
 0.0675
0.0749
0.0967
0.1249
 0.1445
0.1544
0.1601
0.1617

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0439
0.0392
0.0309
0.0243
 0.0535
0.095
0.1772
0.2646
 0.0571
0.0697
0.0978
0.1259
 0.1109
0.1195
0.1256
0.1273

Runing Epoch 005 train loss 4414.0356 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 006 train loss 4156.4144 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 007 train loss 3910.7004 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 008 train loss 3699.6277 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 009 train loss 3510.9745 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0638
0.0594
0.0501
0.0417
 0.0494
0.0883
0.1708
0.2623
 0.0739
0.0841
0.1119
0.1462
 0.1537
0.1645
0.1709
0.1723

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0524
0.0468
0.0375
0.03
 0.0712
0.1225
0.2284
0.3415
 0.0711
0.0879
0.1246
0.161
 0.1298
0.1399
0.1465
0.1481

Runing Epoch 010 train loss 3355.7226 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 011 train loss 3211.2174 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 012 train loss 3077.8561 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 013 train loss 2949.9562 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 014 train loss 2855.6570 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0663
0.0612
0.052
0.0443
 0.0534
0.0958
0.1891
0.2951
 0.0775
0.0887
0.1205
0.1596
 0.1584
0.1699
0.1764
0.1779

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0577
0.0517
0.0417
0.0329
 0.0821
0.1409
0.2683
0.3948
 0.0811
0.1008
0.1443
0.1843
 0.1468
0.1577
0.1646
0.1661

Runing Epoch 015 train loss 2756.1825 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 016 train loss 2665.0287 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 017 train loss 2591.0161 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 018 train loss 2524.0380 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 019 train loss 2454.3346 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0665
0.0616
0.0531
0.0451
 0.0576
0.1006
0.2031
0.3128
 0.0788
0.0912
0.1261
0.1664
 0.1583
0.1701
0.1771
0.1785

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0601
0.0538
0.0438
0.0344
 0.0906
0.1539
0.2927
0.4235
 0.0869
0.1085
0.1558
0.1971
 0.1539
0.1655
0.1726
0.174

Runing Epoch 020 train loss 2390.2400 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 021 train loss 2332.3073 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 022 train loss 2282.6485 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 023 train loss 2234.9953 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 024 train loss 2195.0099 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0664
0.0617
0.0534
0.0458
 0.0586
0.1041
0.2088
0.3256
 0.0798
0.0931
0.129
0.1714
 0.1598
0.1719
0.1789
0.1803

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0617
0.0555
0.0448
0.035
 0.096
0.1646
0.3069
0.4399
 0.0903
0.1139
0.1622
0.2043
 0.1572
0.1694
0.1763
0.1777

Runing Epoch 025 train loss 2158.0372 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 026 train loss 2112.5592 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 027 train loss 2077.1219 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 028 train loss 2049.6264 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 029 train loss 2012.7070 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0674
0.0619
0.0537
0.0458
 0.0615
0.1068
0.2139
0.3323
 0.0804
0.0937
0.1305
0.1733
 0.1568
0.1689
0.176
0.1775

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0635
0.0564
0.0458
0.0357
 0.1016
0.1714
0.3188
0.4566
 0.0932
0.1172
0.1675
0.2107
 0.1579
0.1704
0.1774
0.1788

Runing Epoch 030 train loss 1986.6034 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 031 train loss 1955.7573 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 032 train loss 1933.5659 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 033 train loss 1905.9396 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 034 train loss 1883.2721 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0665
0.061
0.0533
0.0454
 0.062
0.1077
0.2153
0.3324
 0.0808
0.0942
0.1315
0.1738
 0.1588
0.1709
0.178
0.1795

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0638
0.0576
0.0459
0.0357
 0.1056
0.1778
0.3247
0.461
 0.0957
0.121
0.1708
0.2137
 0.1625
0.175
0.1818
0.1832

Runing Epoch 035 train loss 1866.0092 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 036 train loss 1837.5973 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 037 train loss 1817.9271 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 038 train loss 1801.8093 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 039 train loss 1784.1857 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0669
0.0612
0.0533
0.0456
 0.0637
0.1092
0.2184
0.3373
 0.0813
0.0947
0.1323
0.1752
 0.1592
0.1708
0.178
0.1795

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.064
0.0572
0.0459
0.0361
 0.1081
0.18
0.3284
0.4719
 0.0961
0.1212
0.1715
0.2163
 0.1613
0.1736
0.1805
0.182

Runing Epoch 040 train loss 1771.5540 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 041 train loss 1747.7383 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 042 train loss 1733.9352 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 043 train loss 1719.2462 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 044 train loss 1706.7982 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0659
0.0607
0.0532
0.0454
 0.0629
0.1088
0.2189
0.3373
 0.08
0.0937
0.1319
0.1747
 0.1553
0.1674
0.1748
0.1763

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.064
0.0573
0.046
0.0362
 0.1082
0.1821
0.3299
0.4751
 0.0969
0.1223
0.1728
0.2179
 0.1638
0.1761
0.1829
0.1844

Runing Epoch 045 train loss 1690.5261 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 046 train loss 1682.9099 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 047 train loss 1663.9627 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 048 train loss 1652.4052 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 049 train loss 1637.7058 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0672
0.061
0.0535
0.0459
 0.0645
0.1116
0.2213
0.3414
 0.0825
0.0959
0.1339
0.1774
 0.1621
0.174
0.1812
0.1827

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0647
0.0577
0.047
0.0366
 0.1097
0.1847
0.3394
0.4805
 0.0981
0.124
0.1766
0.2209
 0.1648
0.1775
0.1843
0.1857

Runing Epoch 050 train loss 1627.8466 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 051 train loss 1614.1818 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 052 train loss 1610.7465 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 053 train loss 1597.3993 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 054 train loss 1587.9596 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.066
0.0607
0.0531
0.0453
 0.0642
0.1111
0.2218
0.3404
 0.0812
0.0952
0.1333
0.1762
 0.1595
0.1716
0.1789
0.1805

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0636
0.0576
0.0467
0.0365
 0.1094
0.1872
0.342
0.4841
 0.0966
0.1237
0.1763
0.2208
 0.1612
0.1742
0.181
0.1825

Runing Epoch 055 train loss 1572.8477 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 056 train loss 1570.0028 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 057 train loss 1563.1329 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 058 train loss 1549.6828 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 059 train loss 1543.1677 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0652
0.0602
0.0531
0.0453
 0.0627
0.1101
0.2225
0.3404
 0.0803
0.0944
0.1333
0.176
 0.1588
0.1714
0.1787
0.1802

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0644
0.0575
0.0468
0.0366
 0.1111
0.1862
0.3412
0.4854
 0.0984
0.1245
0.1772
0.2223
 0.164
0.1766
0.1835
0.1849

Runing Epoch 060 train loss 1533.7752 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 061 train loss 1527.8483 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 062 train loss 1518.5959 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 063 train loss 1508.2484 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 064 train loss 1503.4596 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0659
0.0606
0.0534
0.0457
 0.0639
0.1113
0.2242
0.3445
 0.0808
0.095
0.1341
0.1775
 0.1576
0.17
0.1774
0.1789

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0653
0.0578
0.047
0.0367
 0.112
0.1883
0.3438
0.4884
 0.099
0.1252
0.1782
0.2234
 0.1641
0.1765
0.1834
0.1849

Runing Epoch 065 train loss 1494.4175 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 066 train loss 1488.6037 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 067 train loss 1481.8099 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 068 train loss 1475.6156 costs 00: 00: 00
------------------------------------------------------
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÜ
wandb:         batch_loss_train ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         best_test MRR@10 ‚ñÅ
wandb:        best_test MRR@100 ‚ñÅ
wandb:         best_test MRR@20 ‚ñÅ
wandb:         best_test MRR@50 ‚ñÅ
wandb:        best_test NDCG@10 ‚ñÅ
wandb:       best_test NDCG@100 ‚ñÅ
wandb:        best_test NDCG@20 ‚ñÅ
wandb:        best_test NDCG@50 ‚ñÅ
wandb:   best_test Precision@10 ‚ñÅ
wandb:  best_test Precision@100 ‚ñÅ
wandb:   best_test Precision@20 ‚ñÅ
wandb:   best_test Precision@50 ‚ñÅ
wandb:      best_test Recall@10 ‚ñÅ
wandb:     best_test Recall@100 ‚ñÅ
wandb:      best_test Recall@20 ‚ñÅ
wandb:      best_test Recall@50 ‚ñÅ
wandb:        best_valid MRR@10 ‚ñÅ
wandb:       best_valid MRR@100 ‚ñÅ
wandb:        best_valid MRR@20 ‚ñÅ
wandb:        best_valid MRR@50 ‚ñÅ
wandb:       best_valid NDCG@10 ‚ñÅ
wandb:      best_valid NDCG@100 ‚ñÅ
wandb:       best_valid NDCG@20 ‚ñÅ
wandb:       best_valid NDCG@50 ‚ñÅ
wandb:  best_valid Precision@10 ‚ñÅ
wandb: best_valid Precision@100 ‚ñÅ
wandb:  best_valid Precision@20 ‚ñÅ
wandb:  best_valid Precision@50 ‚ñÅ
wandb:     best_valid Recall@10 ‚ñÅ
wandb:    best_valid Recall@100 ‚ñÅ
wandb:     best_valid Recall@20 ‚ñÅ
wandb:     best_valid Recall@50 ‚ñÅ
wandb:    epoch_loss_norm_train ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              test MRR@10 ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test MRR@100 ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:              test MRR@20 ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:              test MRR@50 ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test NDCG@10 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:            test NDCG@100 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test NDCG@20 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test NDCG@50 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test Precision@10 ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà
wandb:       test Precision@100 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test Precision@20 ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test Precision@50 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           test Recall@10 ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:          test Recall@100 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           test Recall@20 ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           test Recall@50 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:             valid MRR@10 ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñÜ
wandb:            valid MRR@100 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá
wandb:             valid MRR@20 ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá
wandb:             valid MRR@50 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá
wandb:            valid NDCG@10 ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá
wandb:           valid NDCG@100 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:            valid NDCG@20 ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:            valid NDCG@50 ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       valid Precision@10 ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñá
wandb:      valid Precision@100 ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       valid Precision@20 ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá
wandb:       valid Precision@50 ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:          valid Recall@10 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà
wandb:         valid Recall@100 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:          valid Recall@20 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:          valid Recall@50 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:                    Epoch 50
wandb:         batch_loss_train 97.64525
wandb:         best_test MRR@10 0.1648
wandb:        best_test MRR@100 0.1857
wandb:         best_test MRR@20 0.1775
wandb:         best_test MRR@50 0.1843
wandb:        best_test NDCG@10 0.0981
wandb:       best_test NDCG@100 0.2209
wandb:        best_test NDCG@20 0.124
wandb:        best_test NDCG@50 0.1766
wandb:   best_test Precision@10 0.0647
wandb:  best_test Precision@100 0.0366
wandb:   best_test Precision@20 0.0577
wandb:   best_test Precision@50 0.047
wandb:      best_test Recall@10 0.1097
wandb:     best_test Recall@100 0.4805
wandb:      best_test Recall@20 0.1847
wandb:      best_test Recall@50 0.3394
wandb:        best_valid MRR@10 0.1621
wandb:       best_valid MRR@100 0.1827
wandb:        best_valid MRR@20 0.174
wandb:        best_valid MRR@50 0.1812
wandb:       best_valid NDCG@10 0.0825
wandb:      best_valid NDCG@100 0.1774
wandb:       best_valid NDCG@20 0.0959
wandb:       best_valid NDCG@50 0.1339
wandb:  best_valid Precision@10 0.0672
wandb: best_valid Precision@100 0.0459
wandb:  best_valid Precision@20 0.061
wandb:  best_valid Precision@50 0.0535
wandb:     best_valid Recall@10 0.0645
wandb:    best_valid Recall@100 0.3414
wandb:     best_valid Recall@20 0.1116
wandb:     best_valid Recall@50 0.2213
wandb:    epoch_loss_norm_train 97.9826
wandb:              test MRR@10 0.1641
wandb:             test MRR@100 0.1849
wandb:              test MRR@20 0.1765
wandb:              test MRR@50 0.1834
wandb:             test NDCG@10 0.099
wandb:            test NDCG@100 0.2234
wandb:             test NDCG@20 0.1252
wandb:             test NDCG@50 0.1782
wandb:        test Precision@10 0.0653
wandb:       test Precision@100 0.0367
wandb:        test Precision@20 0.0578
wandb:        test Precision@50 0.047
wandb:           test Recall@10 0.112
wandb:          test Recall@100 0.4884
wandb:           test Recall@20 0.1883
wandb:           test Recall@50 0.3438
wandb:             valid MRR@10 0.1576
wandb:            valid MRR@100 0.1789
wandb:             valid MRR@20 0.17
wandb:             valid MRR@50 0.1774
wandb:            valid NDCG@10 0.0808
wandb:           valid NDCG@100 0.1775
wandb:            valid NDCG@20 0.095
wandb:            valid NDCG@50 0.1341
wandb:       valid Precision@10 0.0659
wandb:      valid Precision@100 0.0457
wandb:       valid Precision@20 0.0606
wandb:       valid Precision@50 0.0534
wandb:          valid Recall@10 0.0639
wandb:         valid Recall@100 0.3445
wandb:          valid Recall@20 0.1113
wandb:          valid Recall@50 0.2242
wandb: 
wandb: üöÄ View run T-DiffRec_ml-1m_clean_1_Visualization at: https://wandb.ai/drs/drs/runs/1bj5xew6
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230623_153547-1bj5xew6/logs
Runing Epoch 069 train loss 1469.7390 costs 00: 00: 00
------------------------------------------------------
------------------
Exiting from training early
======================================================
End. Best Epoch 050 
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0672
0.061
0.0535
0.0459
 0.0645
0.1116
0.2213
0.3414
 0.0825
0.0959
0.1339
0.1774
 0.1621
0.174
0.1812
0.1827

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0647
0.0577
0.047
0.0366
 0.1097
0.1847
0.3394
0.4805
 0.0981
0.124
0.1766
0.2209
 0.1648
0.1775
0.1843
0.1857

End time:  2023-06-23 15:37:24
params_per_batch.shape: torch.Size([1034, 5, 10])
Saved params_per_batch.npy to mPHATE/T-DiffRec_ml-1m_clean_1_Visualization_params_per_batch.npy
Data shape: (1034, 5, 10)
Calculating M-PHATE...
  Calculating multislice kernel...
  Calculated multislice kernel in 4.31 seconds.
  Calculating graph and diffusion operator...
    Calculating landmark operator...
      Calculating SVD...
      Calculated SVD in 1.03 seconds.
      Calculating KMeans...
      Calculated KMeans in 4.88 seconds.
    Calculated landmark operator in 8.25 seconds.
  Calculated graph and diffusion operator in 8.38 seconds.
  Running PHATE on precomputed affinity matrix with 5170 observations.
  Calculating optimal t...
    Automatically selected t = 30
  Calculated optimal t in 6.93 seconds.
  Calculating diffusion potential...
  Calculated diffusion potential in 3.92 seconds.
  Calculating metric MDS...
  Calculated metric MDS in 27.01 seconds.
Calculated M-PHATE in 50.57 seconds.
/home/lcur2470/.conda/envs/rs/lib/python3.8/site-packages/sklearn/manifold/_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.
  warnings.warn(
m-PHATE done
Plotting ...
