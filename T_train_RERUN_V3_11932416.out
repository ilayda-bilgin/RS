wandb: Currently logged in as: piconda (drs). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/lcur2470/RS/wandb/run-20230619_225828-g6rbv7r2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run T-DiffRec_yelp_noisy_1_V3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/drs/drs
wandb: üöÄ View run at https://wandb.ai/drs/drs/runs/g6rbv7r2
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÜ
wandb:         batch_loss_train ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ
wandb:         best_test MRR@10 ‚ñÅ
wandb:        best_test MRR@100 ‚ñÅ
wandb:         best_test MRR@20 ‚ñÅ
wandb:         best_test MRR@50 ‚ñÅ
wandb:        best_test NDCG@10 ‚ñÅ
wandb:       best_test NDCG@100 ‚ñÅ
wandb:        best_test NDCG@20 ‚ñÅ
wandb:        best_test NDCG@50 ‚ñÅ
wandb:   best_test Precision@10 ‚ñÅ
wandb:  best_test Precision@100 ‚ñÅ
wandb:   best_test Precision@20 ‚ñÅ
wandb:   best_test Precision@50 ‚ñÅ
wandb:      best_test Recall@10 ‚ñÅ
wandb:     best_test Recall@100 ‚ñÅ
wandb:      best_test Recall@20 ‚ñÅ
wandb:      best_test Recall@50 ‚ñÅ
wandb:        best_valid MRR@10 ‚ñÅ
wandb:       best_valid MRR@100 ‚ñÅ
wandb:        best_valid MRR@20 ‚ñÅ
wandb:        best_valid MRR@50 ‚ñÅ
wandb:       best_valid NDCG@10 ‚ñÅ
wandb:      best_valid NDCG@100 ‚ñÅ
wandb:       best_valid NDCG@20 ‚ñÅ
wandb:       best_valid NDCG@50 ‚ñÅ
wandb:  best_valid Precision@10 ‚ñÅ
wandb: best_valid Precision@100 ‚ñÅ
wandb:  best_valid Precision@20 ‚ñÅ
wandb:  best_valid Precision@50 ‚ñÅ
wandb:     best_valid Recall@10 ‚ñÅ
wandb:    best_valid Recall@100 ‚ñÅ
wandb:     best_valid Recall@20 ‚ñÅ
wandb:     best_valid Recall@50 ‚ñÅ
wandb:    epoch_loss_norm_train ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              test MRR@10 ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá
wandb:             test MRR@100 ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:              test MRR@20 ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:              test MRR@50 ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:             test NDCG@10 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:            test NDCG@100 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:             test NDCG@20 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:             test NDCG@50 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:        test Precision@10 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñÜ
wandb:       test Precision@100 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá
wandb:        test Precision@20 ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:        test Precision@50 ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá
wandb:           test Recall@10 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá
wandb:          test Recall@100 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:           test Recall@20 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:           test Recall@50 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:             valid MRR@10 ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá
wandb:            valid MRR@100 ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá
wandb:             valid MRR@20 ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá
wandb:             valid MRR@50 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá
wandb:            valid NDCG@10 ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:           valid NDCG@100 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:            valid NDCG@20 ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:            valid NDCG@50 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:       valid Precision@10 ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá
wandb:      valid Precision@100 ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá
wandb:       valid Precision@20 ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:       valid Precision@50 ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:          valid Recall@10 ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:         valid Recall@100 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:          valid Recall@20 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:          valid Recall@50 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá
wandb: 
wandb: Run summary:
wandb:                    Epoch 55
wandb:         batch_loss_train 1.57097
wandb:         best_test MRR@10 0.0349
wandb:        best_test MRR@100 0.0437
wandb:         best_test MRR@20 0.0388
wandb:         best_test MRR@50 0.0422
wandb:        best_test NDCG@10 0.0285
wandb:       best_test NDCG@100 0.0671
wandb:        best_test NDCG@20 0.0374
wandb:        best_test NDCG@50 0.0527
wandb:   best_test Precision@10 0.0108
wandb:  best_test Precision@100 0.0052
wandb:   best_test Precision@20 0.009
wandb:   best_test Precision@50 0.0067
wandb:      best_test Recall@10 0.0461
wandb:     best_test Recall@100 0.2076
wandb:      best_test Recall@20 0.0754
wandb:      best_test Recall@50 0.1375
wandb:        best_valid MRR@10 0.04
wandb:       best_valid MRR@100 0.0497
wandb:        best_valid MRR@20 0.0445
wandb:        best_valid MRR@50 0.0482
wandb:       best_valid NDCG@10 0.0247
wandb:      best_valid NDCG@100 0.0591
wandb:       best_valid NDCG@20 0.0319
wandb:       best_valid NDCG@50 0.0456
wandb:  best_valid Precision@10 0.0127
wandb: best_valid Precision@100 0.0067
wandb:  best_valid Precision@20 0.0108
wandb:  best_valid Precision@50 0.0085
wandb:     best_valid Recall@10 0.0342
wandb:    best_valid Recall@100 0.1636
wandb:     best_valid Recall@20 0.057
wandb:     best_valid Recall@50 0.1064
wandb:    epoch_loss_norm_train 1.711
wandb:              test MRR@10 0.0339
wandb:             test MRR@100 0.0426
wandb:              test MRR@20 0.0379
wandb:              test MRR@50 0.0412
wandb:             test NDCG@10 0.0276
wandb:            test NDCG@100 0.0653
wandb:             test NDCG@20 0.0365
wandb:             test NDCG@50 0.0513
wandb:        test Precision@10 0.0104
wandb:       test Precision@100 0.0051
wandb:        test Precision@20 0.0088
wandb:        test Precision@50 0.0066
wandb:           test Recall@10 0.0448
wandb:          test Recall@100 0.2017
wandb:           test Recall@20 0.0738
wandb:           test Recall@50 0.1336
wandb:             valid MRR@10 0.0392
wandb:            valid MRR@100 0.0487
wandb:             valid MRR@20 0.0435
wandb:             valid MRR@50 0.0472
wandb:            valid NDCG@10 0.0242
wandb:           valid NDCG@100 0.0576
wandb:            valid NDCG@20 0.031
wandb:            valid NDCG@50 0.0445
wandb:       valid Precision@10 0.0124
wandb:      valid Precision@100 0.0066
wandb:       valid Precision@20 0.0105
wandb:       valid Precision@50 0.0083
wandb:          valid Recall@10 0.0335
wandb:         valid Recall@100 0.1591
wandb:          valid Recall@20 0.0552
wandb:          valid Recall@50 0.1041
wandb: 
wandb: üöÄ View run T-DiffRec_yelp_noisy_1_V3 at: https://wandb.ai/drs/drs/runs/g6rbv7r2
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230619_225828-g6rbv7r2/logs
args: Namespace(batch_size=400, cuda=True, data_path='./datasets/', dataset='yelp_noisy', dims='[1000]', emb_size=10, epochs=1000, gpu='1', log_name='log', lr=0.0001, mean_type='x0', model_type='T-DiffRec', noise_max=0.02, noise_min=0.0001, noise_scale=0.1, noise_schedule='linear-var', norm=False, num_workers=0, reweight=True, round=1, run_name='V3', sampling_noise=False, sampling_steps=0, save_path='./saved_models/', seed=1, steps=5, time_type='cat', topN='[10, 20, 50, 100]', tst_w_val=False, w_max=1.0, w_min=0.1, weight_decay=0.0)
Starting time:  2023-06-19 22:58:32
user num: 54574
item num: 77405
data ready.
models ready.
Number of all parameters: 154898565
Start training...
Runing Epoch 001 train loss 329.5605 costs 00: 00: 16
------------------------------------------------------
Runing Epoch 002 train loss 268.4050 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 003 train loss 260.4719 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 004 train loss 257.4595 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0104
0.009
0.007
0.0057
 0.0279
0.0463
0.0859
0.1339
 0.0204
0.0262
0.0372
0.0487
 0.0335
0.0372
0.0404
0.0418

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0086
0.0071
0.0054
0.0043
 0.0365
0.0586
0.1081
0.1666
 0.0228
0.0296
0.0419
0.0539
 0.0285
0.0316
0.0345
0.0358

Runing Epoch 005 train loss 255.6226 costs 00: 01: 10
------------------------------------------------------
Runing Epoch 006 train loss 253.7328 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 007 train loss 252.3971 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 008 train loss 250.6508 costs 00: 00: 16
------------------------------------------------------
Runing Epoch 009 train loss 249.1033 costs 00: 00: 16
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0121
0.0103
0.008
0.0065
 0.0321
0.053
0.0981
0.1525
 0.0232
0.0298
0.0424
0.0553
 0.0377
0.0419
0.0454
0.0469

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0099
0.0083
0.0063
0.0049
 0.0421
0.0684
0.1255
0.1914
 0.0263
0.0344
0.0486
0.0622
 0.0331
0.0367
0.0399
0.0413

Runing Epoch 010 train loss 247.4949 costs 00: 01: 12
------------------------------------------------------
Runing Epoch 011 train loss 246.2970 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 012 train loss 245.1639 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 013 train loss 244.1032 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 014 train loss 243.1753 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0123
0.0106
0.0083
0.0066
 0.0331
0.0547
0.1018
0.1556
 0.0239
0.0307
0.0438
0.0567
 0.0383
0.0427
0.0463
0.0477

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0103
0.0086
0.0065
0.005
 0.0441
0.0712
0.1301
0.196
 0.0274
0.0357
0.0504
0.064
 0.0339
0.0377
0.0409
0.0423

Runing Epoch 015 train loss 241.9451 costs 00: 01: 11
------------------------------------------------------
Runing Epoch 016 train loss 240.9745 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 017 train loss 240.5869 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 018 train loss 239.5280 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 019 train loss 239.0511 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0125
0.0107
0.0084
0.0067
 0.0332
0.0549
0.1031
0.1588
 0.0243
0.0311
0.0445
0.0577
 0.0393
0.0438
0.0473
0.0488

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0105
0.0088
0.0066
0.0051
 0.0448
0.0734
0.1335
0.2003
 0.0279
0.0366
0.0516
0.0654
 0.0345
0.0383
0.0416
0.0431

Runing Epoch 020 train loss 238.5459 costs 00: 01: 58
------------------------------------------------------
Runing Epoch 021 train loss 238.1944 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 022 train loss 237.7035 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 023 train loss 237.1675 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 024 train loss 236.8044 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0125
0.0108
0.0084
0.0067
 0.0335
0.0559
0.1041
0.1598
 0.0242
0.0313
0.0447
0.0579
 0.039
0.0434
0.047
0.0485

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0104
0.0089
0.0066
0.0051
 0.0445
0.0733
0.1338
0.2018
 0.0278
0.0367
0.0517
0.0657
 0.0346
0.0386
0.0419
0.0434

Runing Epoch 025 train loss 236.9837 costs 00: 01: 11
------------------------------------------------------
Runing Epoch 026 train loss 236.6820 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 027 train loss 236.2678 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 028 train loss 235.7939 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 029 train loss 235.8195 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0124
0.0107
0.0084
0.0067
 0.0333
0.0559
0.1046
0.1607
 0.0241
0.0313
0.0448
0.0582
 0.0392
0.0437
0.0473
0.0488

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0106
0.0089
0.0066
0.0051
 0.0453
0.0738
0.1333
0.2026
 0.0279
0.0366
0.0514
0.0657
 0.0343
0.0382
0.0415
0.043

Runing Epoch 030 train loss 235.6821 costs 00: 01: 09
------------------------------------------------------
Runing Epoch 031 train loss 235.5928 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 032 train loss 235.4116 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 033 train loss 235.5456 costs 00: 00: 16
------------------------------------------------------
Runing Epoch 034 train loss 235.1046 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0125
0.0108
0.0085
0.0068
 0.0337
0.0562
0.1063
0.1627
 0.0245
0.0317
0.0455
0.0589
 0.04
0.0445
0.0482
0.0497

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0107
0.009
0.0067
0.0052
 0.0463
0.0751
0.1358
0.2061
 0.0285
0.0373
0.0524
0.0669
 0.035
0.0388
0.0422
0.0436

Runing Epoch 035 train loss 235.3591 costs 00: 01: 50
------------------------------------------------------
Runing Epoch 036 train loss 235.0529 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 037 train loss 235.0122 costs 00: 00: 16
------------------------------------------------------
Runing Epoch 038 train loss 235.0037 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 039 train loss 234.8510 costs 00: 00: 16
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0126
0.0108
0.0085
0.0068
 0.0337
0.0566
0.1056
0.1629
 0.0244
0.0316
0.0452
0.0587
 0.0397
0.0441
0.0478
0.0493

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0108
0.009
0.0067
0.0051
 0.0458
0.0748
0.1357
0.2043
 0.0283
0.0371
0.0522
0.0663
 0.0348
0.0387
0.042
0.0435

Runing Epoch 040 train loss 234.9371 costs 00: 01: 15
------------------------------------------------------
Runing Epoch 041 train loss 234.9076 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 042 train loss 234.7334 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 043 train loss 234.8759 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 044 train loss 234.9175 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0126
0.0108
0.0085
0.0068
 0.0339
0.0568
0.1066
0.1641
 0.0245
0.0317
0.0455
0.0591
 0.0394
0.044
0.0477
0.0492

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0108
0.009
0.0068
0.0052
 0.0462
0.0752
0.1371
0.2077
 0.0287
0.0376
0.0529
0.0674
 0.0352
0.0391
0.0425
0.0439

Runing Epoch 045 train loss 234.7598 costs 00: 01: 15
------------------------------------------------------
Runing Epoch 046 train loss 234.4104 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 047 train loss 234.5802 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 048 train loss 234.5466 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 049 train loss 234.4048 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0126
0.0108
0.0085
0.0068
 0.0341
0.0565
0.1071
0.1644
 0.0247
0.0318
0.0457
0.0593
 0.0399
0.0444
0.0482
0.0497

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0108
0.009
0.0067
0.0052
 0.0464
0.0759
0.1377
0.2088
 0.0287
0.0377
0.053
0.0676
 0.0353
0.0392
0.0426
0.0441

Runing Epoch 050 train loss 234.3596 costs 00: 01: 09
------------------------------------------------------
Runing Epoch 051 train loss 234.7696 costs 00: 00: 16
------------------------------------------------------
Runing Epoch 052 train loss 234.6882 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 053 train loss 234.6141 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 054 train loss 234.3127 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0127
0.0108
0.0085
0.0067
 0.0342
0.057
0.1064
0.1636
 0.0247
0.0319
0.0456
0.0591
 0.04
0.0445
0.0482
0.0497

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0108
0.009
0.0067
0.0052
 0.0461
0.0754
0.1375
0.2076
 0.0285
0.0374
0.0527
0.0671
 0.0349
0.0388
0.0422
0.0437

Runing Epoch 055 train loss 234.3402 costs 00: 01: 12
------------------------------------------------------
Runing Epoch 056 train loss 234.5016 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 057 train loss 234.4172 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 058 train loss 234.5151 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 059 train loss 234.4881 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0128
0.0109
0.0085
0.0068
 0.0342
0.0567
0.1055
0.1648
 0.0246
0.0317
0.0452
0.0591
 0.0394
0.0438
0.0475
0.049

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0106
0.009
0.0068
0.0053
 0.0455
0.0753
0.137
0.2086
 0.0283
0.0374
0.0527
0.0674
 0.0347
0.0387
0.0421
0.0436

Runing Epoch 060 train loss 234.6088 costs 00: 01: 09
------------------------------------------------------
Runing Epoch 061 train loss 234.7031 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 062 train loss 234.6099 costs 00: 00: 16
------------------------------------------------------
Runing Epoch 063 train loss 234.2972 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 064 train loss 234.2824 costs 00: 00: 15
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0127
0.0108
0.0085
0.0068
 0.0339
0.057
0.1074
0.1655
 0.0246
0.0318
0.0457
0.0594
 0.0396
0.0441
0.0479
0.0494

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0109
0.009
0.0067
0.0052
 0.0469
0.0755
0.1383
0.2094
 0.0287
0.0374
0.053
0.0675
 0.0351
0.0389
0.0423
0.0438

Runing Epoch 065 train loss 234.3495 costs 00: 01: 09
------------------------------------------------------
Runing Epoch 066 train loss 234.1619 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 067 train loss 234.2070 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 068 train loss 234.4167 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 069 train loss 234.5604 costs 00: 00: 16
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0124
0.0105
0.0083
0.0066
 0.0335
0.0552
0.1041
0.1591
 0.0242
0.031
0.0445
0.0576
 0.0392
0.0435
0.0472
0.0487

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0104
0.0088
0.0066
0.0051
 0.0448
0.0738
0.1336
0.2017
 0.0276
0.0365
0.0513
0.0653
 0.0339
0.0379
0.0412
0.0426

Runing Epoch 070 train loss 234.4655 costs 00: 01: 09
------------------------------------------------------
Runing Epoch 071 train loss 234.3398 costs 00: 00: 16
------------------------------------------------------
Runing Epoch 072 train loss 234.5349 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 073 train loss 234.2268 costs 00: 00: 15
------------------------------------------------------
Runing Epoch 074 train loss 234.4069 costs 00: 00: 15
------------------------------------------------------
------------------
Exiting from training early
======================================================
End. Best Epoch 055 
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0127
0.0108
0.0085
0.0067
 0.0342
0.057
0.1064
0.1636
 0.0247
0.0319
0.0456
0.0591
 0.04
0.0445
0.0482
0.0497

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0108
0.009
0.0067
0.0052
 0.0461
0.0754
0.1375
0.2076
 0.0285
0.0374
0.0527
0.0671
 0.0349
0.0388
0.0422
0.0437

End time:  2023-06-19 23:32:39
wandb: Currently logged in as: piconda (drs). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/lcur2470/RS/wandb/run-20230619_233252-s7m4cooj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run T-DiffRec_ml-1m_clean_1_V3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/drs/drs
wandb: üöÄ View run at https://wandb.ai/drs/drs/runs/s7m4cooj
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.004 MB of 0.036 MB uploaded (0.000 MB deduped)wandb: \ 0.004 MB of 0.036 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                    Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñÜ
wandb:         batch_loss_train ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         best_test MRR@10 ‚ñÅ
wandb:        best_test MRR@100 ‚ñÅ
wandb:         best_test MRR@20 ‚ñÅ
wandb:         best_test MRR@50 ‚ñÅ
wandb:        best_test NDCG@10 ‚ñÅ
wandb:       best_test NDCG@100 ‚ñÅ
wandb:        best_test NDCG@20 ‚ñÅ
wandb:        best_test NDCG@50 ‚ñÅ
wandb:   best_test Precision@10 ‚ñÅ
wandb:  best_test Precision@100 ‚ñÅ
wandb:   best_test Precision@20 ‚ñÅ
wandb:   best_test Precision@50 ‚ñÅ
wandb:      best_test Recall@10 ‚ñÅ
wandb:     best_test Recall@100 ‚ñÅ
wandb:      best_test Recall@20 ‚ñÅ
wandb:      best_test Recall@50 ‚ñÅ
wandb:        best_valid MRR@10 ‚ñÅ
wandb:       best_valid MRR@100 ‚ñÅ
wandb:        best_valid MRR@20 ‚ñÅ
wandb:        best_valid MRR@50 ‚ñÅ
wandb:       best_valid NDCG@10 ‚ñÅ
wandb:      best_valid NDCG@100 ‚ñÅ
wandb:       best_valid NDCG@20 ‚ñÅ
wandb:       best_valid NDCG@50 ‚ñÅ
wandb:  best_valid Precision@10 ‚ñÅ
wandb: best_valid Precision@100 ‚ñÅ
wandb:  best_valid Precision@20 ‚ñÅ
wandb:  best_valid Precision@50 ‚ñÅ
wandb:     best_valid Recall@10 ‚ñÅ
wandb:    best_valid Recall@100 ‚ñÅ
wandb:     best_valid Recall@20 ‚ñÅ
wandb:     best_valid Recall@50 ‚ñÅ
wandb:    epoch_loss_norm_train ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              test MRR@10 ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test MRR@100 ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:              test MRR@20 ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:              test MRR@50 ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test NDCG@10 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:            test NDCG@100 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test NDCG@20 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test NDCG@50 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test Precision@10 ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà
wandb:       test Precision@100 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test Precision@20 ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test Precision@50 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           test Recall@10 ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:          test Recall@100 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           test Recall@20 ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           test Recall@50 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:             valid MRR@10 ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñÜ
wandb:            valid MRR@100 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá
wandb:             valid MRR@20 ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá
wandb:             valid MRR@50 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá
wandb:            valid NDCG@10 ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá
wandb:           valid NDCG@100 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:            valid NDCG@20 ‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:            valid NDCG@50 ‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       valid Precision@10 ‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñá
wandb:      valid Precision@100 ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       valid Precision@20 ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá
wandb:       valid Precision@50 ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:          valid Recall@10 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà
wandb:         valid Recall@100 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:          valid Recall@20 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:          valid Recall@50 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:                    Epoch 50
wandb:         batch_loss_train 97.64525
wandb:         best_test MRR@10 0.1648
wandb:        best_test MRR@100 0.1857
wandb:         best_test MRR@20 0.1775
wandb:         best_test MRR@50 0.1843
wandb:        best_test NDCG@10 0.0981
wandb:       best_test NDCG@100 0.2209
wandb:        best_test NDCG@20 0.124
wandb:        best_test NDCG@50 0.1766
wandb:   best_test Precision@10 0.0647
wandb:  best_test Precision@100 0.0366
wandb:   best_test Precision@20 0.0577
wandb:   best_test Precision@50 0.047
wandb:      best_test Recall@10 0.1097
wandb:     best_test Recall@100 0.4805
wandb:      best_test Recall@20 0.1847
wandb:      best_test Recall@50 0.3394
wandb:        best_valid MRR@10 0.1621
wandb:       best_valid MRR@100 0.1827
wandb:        best_valid MRR@20 0.174
wandb:        best_valid MRR@50 0.1812
wandb:       best_valid NDCG@10 0.0825
wandb:      best_valid NDCG@100 0.1774
wandb:       best_valid NDCG@20 0.0959
wandb:       best_valid NDCG@50 0.1339
wandb:  best_valid Precision@10 0.0672
wandb: best_valid Precision@100 0.0459
wandb:  best_valid Precision@20 0.061
wandb:  best_valid Precision@50 0.0535
wandb:     best_valid Recall@10 0.0645
wandb:    best_valid Recall@100 0.3414
wandb:     best_valid Recall@20 0.1116
wandb:     best_valid Recall@50 0.2213
wandb:    epoch_loss_norm_train 97.9826
wandb:              test MRR@10 0.1641
wandb:             test MRR@100 0.1849
wandb:              test MRR@20 0.1765
wandb:              test MRR@50 0.1834
wandb:             test NDCG@10 0.099
wandb:            test NDCG@100 0.2234
wandb:             test NDCG@20 0.1252
wandb:             test NDCG@50 0.1782
wandb:        test Precision@10 0.0653
wandb:       test Precision@100 0.0367
wandb:        test Precision@20 0.0578
wandb:        test Precision@50 0.047
wandb:           test Recall@10 0.112
wandb:          test Recall@100 0.4884
wandb:           test Recall@20 0.1883
wandb:           test Recall@50 0.3438
wandb:             valid MRR@10 0.1576
wandb:            valid MRR@100 0.1789
wandb:             valid MRR@20 0.17
wandb:             valid MRR@50 0.1774
wandb:            valid NDCG@10 0.0808
wandb:           valid NDCG@100 0.1775
wandb:            valid NDCG@20 0.095
wandb:            valid NDCG@50 0.1341
wandb:       valid Precision@10 0.0659
wandb:      valid Precision@100 0.0457
wandb:       valid Precision@20 0.0606
wandb:       valid Precision@50 0.0534
wandb:          valid Recall@10 0.0639
wandb:         valid Recall@100 0.3445
wandb:          valid Recall@20 0.1113
wandb:          valid Recall@50 0.2242
wandb: 
wandb: üöÄ View run T-DiffRec_ml-1m_clean_1_V3 at: https://wandb.ai/drs/drs/runs/s7m4cooj
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230619_233252-s7m4cooj/logs
args: Namespace(batch_size=400, cuda=True, data_path='./datasets/', dataset='ml-1m_clean', dims='[1000]', emb_size=10, epochs=1000, gpu='1', log_name='log', lr=0.0001, mean_type='x0', model_type='T-DiffRec', noise_max=0.02, noise_min=0.0001, noise_scale=0.1, noise_schedule='linear-var', norm=False, num_workers=0, reweight=True, round=1, run_name='V3', sampling_noise=False, sampling_steps=0, save_path='./saved_models/', seed=1, steps=5, time_type='cat', topN='[10, 20, 50, 100]', tst_w_val=False, w_max=1.0, w_min=0.1, weight_decay=0.0)
Starting time:  2023-06-19 23:32:56
user num: 5949
item num: 2810
data ready.
models ready.
Number of all parameters: 5633970
Start training...
Runing Epoch 001 train loss 6309.8419 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 002 train loss 5588.6492 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 003 train loss 5106.6729 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 004 train loss 4735.5258 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0593
0.0547
0.0449
0.0367
 0.0413
0.0745
0.1427
0.2186
 0.0675
0.0749
0.0967
0.1249
 0.1445
0.1544
0.1601
0.1617

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0439
0.0392
0.0309
0.0243
 0.0535
0.095
0.1772
0.2646
 0.0571
0.0697
0.0978
0.1259
 0.1109
0.1195
0.1256
0.1273

Runing Epoch 005 train loss 4414.0356 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 006 train loss 4156.4144 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 007 train loss 3910.7004 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 008 train loss 3699.6277 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 009 train loss 3510.9745 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0638
0.0594
0.0501
0.0417
 0.0494
0.0883
0.1708
0.2623
 0.0739
0.0841
0.1119
0.1462
 0.1537
0.1645
0.1709
0.1723

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0524
0.0468
0.0375
0.03
 0.0712
0.1225
0.2284
0.3415
 0.0711
0.0879
0.1246
0.161
 0.1298
0.1399
0.1465
0.1481

Runing Epoch 010 train loss 3355.7226 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 011 train loss 3211.2174 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 012 train loss 3077.8561 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 013 train loss 2949.9562 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 014 train loss 2855.6570 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0663
0.0612
0.052
0.0443
 0.0534
0.0958
0.1891
0.2951
 0.0775
0.0887
0.1205
0.1596
 0.1584
0.1699
0.1764
0.1779

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0577
0.0517
0.0417
0.0329
 0.0821
0.1409
0.2683
0.3948
 0.0811
0.1008
0.1443
0.1843
 0.1468
0.1577
0.1646
0.1661

Runing Epoch 015 train loss 2756.1825 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 016 train loss 2665.0287 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 017 train loss 2591.0161 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 018 train loss 2524.0380 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 019 train loss 2454.3346 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0665
0.0616
0.0531
0.0451
 0.0576
0.1006
0.2031
0.3128
 0.0788
0.0912
0.1261
0.1664
 0.1583
0.1701
0.1771
0.1785

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0601
0.0538
0.0438
0.0344
 0.0906
0.1539
0.2927
0.4235
 0.0869
0.1085
0.1558
0.1971
 0.1539
0.1655
0.1726
0.174

Runing Epoch 020 train loss 2390.2400 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 021 train loss 2332.3073 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 022 train loss 2282.6485 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 023 train loss 2234.9953 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 024 train loss 2195.0099 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0664
0.0617
0.0534
0.0458
 0.0586
0.1041
0.2088
0.3256
 0.0798
0.0931
0.129
0.1714
 0.1598
0.1719
0.1789
0.1803

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0617
0.0555
0.0448
0.035
 0.096
0.1646
0.3069
0.4399
 0.0903
0.1139
0.1622
0.2043
 0.1572
0.1694
0.1763
0.1777

Runing Epoch 025 train loss 2158.0372 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 026 train loss 2112.5592 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 027 train loss 2077.1219 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 028 train loss 2049.6264 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 029 train loss 2012.7070 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0674
0.0619
0.0537
0.0458
 0.0615
0.1068
0.2139
0.3323
 0.0804
0.0937
0.1305
0.1733
 0.1568
0.1689
0.176
0.1775

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0635
0.0564
0.0458
0.0357
 0.1016
0.1714
0.3188
0.4566
 0.0932
0.1172
0.1675
0.2107
 0.1579
0.1704
0.1774
0.1788

Runing Epoch 030 train loss 1986.6034 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 031 train loss 1955.7573 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 032 train loss 1933.5659 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 033 train loss 1905.9396 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 034 train loss 1883.2721 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0665
0.061
0.0533
0.0454
 0.062
0.1077
0.2153
0.3324
 0.0808
0.0942
0.1315
0.1738
 0.1588
0.1709
0.178
0.1795

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0638
0.0576
0.0459
0.0357
 0.1056
0.1778
0.3247
0.461
 0.0957
0.121
0.1708
0.2137
 0.1625
0.175
0.1818
0.1832

Runing Epoch 035 train loss 1866.0092 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 036 train loss 1837.5973 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 037 train loss 1817.9271 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 038 train loss 1801.8093 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 039 train loss 1784.1857 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0669
0.0612
0.0533
0.0456
 0.0637
0.1092
0.2184
0.3373
 0.0813
0.0947
0.1323
0.1752
 0.1592
0.1708
0.178
0.1795

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.064
0.0572
0.0459
0.0361
 0.1081
0.18
0.3284
0.4719
 0.0961
0.1212
0.1715
0.2163
 0.1613
0.1736
0.1805
0.182

Runing Epoch 040 train loss 1771.5540 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 041 train loss 1747.7383 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 042 train loss 1733.9352 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 043 train loss 1719.2462 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 044 train loss 1706.7982 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0659
0.0607
0.0532
0.0454
 0.0629
0.1088
0.2189
0.3373
 0.08
0.0937
0.1319
0.1747
 0.1553
0.1674
0.1748
0.1763

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.064
0.0573
0.046
0.0362
 0.1082
0.1821
0.3299
0.4751
 0.0969
0.1223
0.1728
0.2179
 0.1638
0.1761
0.1829
0.1844

Runing Epoch 045 train loss 1690.5261 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 046 train loss 1682.9099 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 047 train loss 1663.9627 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 048 train loss 1652.4052 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 049 train loss 1637.7058 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0672
0.061
0.0535
0.0459
 0.0645
0.1116
0.2213
0.3414
 0.0825
0.0959
0.1339
0.1774
 0.1621
0.174
0.1812
0.1827

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0647
0.0577
0.047
0.0366
 0.1097
0.1847
0.3394
0.4805
 0.0981
0.124
0.1766
0.2209
 0.1648
0.1775
0.1843
0.1857

Runing Epoch 050 train loss 1627.8466 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 051 train loss 1614.1818 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 052 train loss 1610.7465 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 053 train loss 1597.3993 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 054 train loss 1587.9596 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.066
0.0607
0.0531
0.0453
 0.0642
0.1111
0.2218
0.3404
 0.0812
0.0952
0.1333
0.1762
 0.1595
0.1716
0.1789
0.1805

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0636
0.0576
0.0467
0.0365
 0.1094
0.1872
0.342
0.4841
 0.0966
0.1237
0.1763
0.2208
 0.1612
0.1742
0.181
0.1825

Runing Epoch 055 train loss 1572.8477 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 056 train loss 1570.0028 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 057 train loss 1563.1329 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 058 train loss 1549.6828 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 059 train loss 1543.1677 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0652
0.0602
0.0531
0.0453
 0.0627
0.1101
0.2225
0.3404
 0.0803
0.0944
0.1333
0.176
 0.1588
0.1714
0.1787
0.1802

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0644
0.0575
0.0468
0.0366
 0.1111
0.1862
0.3412
0.4854
 0.0984
0.1245
0.1772
0.2223
 0.164
0.1766
0.1835
0.1849

Runing Epoch 060 train loss 1533.7752 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 061 train loss 1527.8483 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 062 train loss 1518.5959 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 063 train loss 1508.2484 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 064 train loss 1503.4596 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0659
0.0606
0.0534
0.0457
 0.0639
0.1113
0.2242
0.3445
 0.0808
0.095
0.1341
0.1775
 0.1576
0.17
0.1774
0.1789

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0653
0.0578
0.047
0.0367
 0.112
0.1883
0.3438
0.4884
 0.099
0.1252
0.1782
0.2234
 0.1641
0.1765
0.1834
0.1849

Runing Epoch 065 train loss 1494.4175 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 066 train loss 1488.6037 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 067 train loss 1481.8099 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 068 train loss 1475.6156 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 069 train loss 1469.7390 costs 00: 00: 00
------------------------------------------------------
------------------
Exiting from training early
======================================================
End. Best Epoch 050 
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0672
0.061
0.0535
0.0459
 0.0645
0.1116
0.2213
0.3414
 0.0825
0.0959
0.1339
0.1774
 0.1621
0.174
0.1812
0.1827

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0647
0.0577
0.047
0.0366
 0.1097
0.1847
0.3394
0.4805
 0.0981
0.124
0.1766
0.2209
 0.1648
0.1775
0.1843
0.1857

End time:  2023-06-19 23:34:17
wandb: Currently logged in as: piconda (drs). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/lcur2470/RS/wandb/run-20230619_233428-jiht9fvv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run T-DiffRec_ml-1m_noisy_1_V3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/drs/drs
wandb: üöÄ View run at https://wandb.ai/drs/drs/runs/jiht9fvv
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.004 MB of 0.024 MB uploaded (0.000 MB deduped)wandb: \ 0.037 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: | 0.037 MB of 0.037 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:                    Epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá
wandb:         batch_loss_train ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         best_test MRR@10 ‚ñÅ
wandb:        best_test MRR@100 ‚ñÅ
wandb:         best_test MRR@20 ‚ñÅ
wandb:         best_test MRR@50 ‚ñÅ
wandb:        best_test NDCG@10 ‚ñÅ
wandb:       best_test NDCG@100 ‚ñÅ
wandb:        best_test NDCG@20 ‚ñÅ
wandb:        best_test NDCG@50 ‚ñÅ
wandb:   best_test Precision@10 ‚ñÅ
wandb:  best_test Precision@100 ‚ñÅ
wandb:   best_test Precision@20 ‚ñÅ
wandb:   best_test Precision@50 ‚ñÅ
wandb:      best_test Recall@10 ‚ñÅ
wandb:     best_test Recall@100 ‚ñÅ
wandb:      best_test Recall@20 ‚ñÅ
wandb:      best_test Recall@50 ‚ñÅ
wandb:        best_valid MRR@10 ‚ñÅ
wandb:       best_valid MRR@100 ‚ñÅ
wandb:        best_valid MRR@20 ‚ñÅ
wandb:        best_valid MRR@50 ‚ñÅ
wandb:       best_valid NDCG@10 ‚ñÅ
wandb:      best_valid NDCG@100 ‚ñÅ
wandb:       best_valid NDCG@20 ‚ñÅ
wandb:       best_valid NDCG@50 ‚ñÅ
wandb:  best_valid Precision@10 ‚ñÅ
wandb: best_valid Precision@100 ‚ñÅ
wandb:  best_valid Precision@20 ‚ñÅ
wandb:  best_valid Precision@50 ‚ñÅ
wandb:     best_valid Recall@10 ‚ñÅ
wandb:    best_valid Recall@100 ‚ñÅ
wandb:     best_valid Recall@20 ‚ñÅ
wandb:     best_valid Recall@50 ‚ñÅ
wandb:    epoch_loss_norm_train ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              test MRR@10 ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÖ
wandb:             test MRR@100 ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ
wandb:              test MRR@20 ‚ñÅ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ
wandb:              test MRR@50 ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ
wandb:             test NDCG@10 ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:            test NDCG@100 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test NDCG@20 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             test NDCG@50 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test Precision@10 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá
wandb:       test Precision@100 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test Precision@20 ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        test Precision@50 ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           test Recall@10 ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:          test Recall@100 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           test Recall@20 ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           test Recall@50 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:             valid MRR@10 ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:            valid MRR@100 ‚ñà‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:             valid MRR@20 ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:             valid MRR@50 ‚ñà‚ñà‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ
wandb:            valid NDCG@10 ‚ñà‚ñá‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÖ
wandb:           valid NDCG@100 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:            valid NDCG@20 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà
wandb:            valid NDCG@50 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:       valid Precision@10 ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:      valid Precision@100 ‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:       valid Precision@20 ‚ñÑ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ
wandb:       valid Precision@50 ‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ
wandb:          valid Recall@10 ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà
wandb:         valid Recall@100 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:          valid Recall@20 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà
wandb:          valid Recall@50 ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: 
wandb: Run summary:
wandb:                    Epoch 60
wandb:         batch_loss_train 87.7632
wandb:         best_test MRR@10 0.0919
wandb:        best_test MRR@100 0.1148
wandb:         best_test MRR@20 0.104
wandb:         best_test MRR@50 0.1126
wandb:        best_test NDCG@10 0.0586
wandb:       best_test NDCG@100 0.171
wandb:        best_test NDCG@20 0.0813
wandb:        best_test NDCG@50 0.1283
wandb:   best_test Precision@10 0.037
wandb:  best_test Precision@100 0.028
wandb:   best_test Precision@20 0.0358
wandb:   best_test Precision@50 0.0328
wandb:      best_test Recall@10 0.0781
wandb:     best_test Recall@100 0.4193
wandb:      best_test Recall@20 0.139
wandb:      best_test Recall@50 0.2786
wandb:        best_valid MRR@10 0.0762
wandb:       best_valid MRR@100 0.1
wandb:        best_valid MRR@20 0.0887
wandb:        best_valid MRR@50 0.0979
wandb:       best_valid NDCG@10 0.0391
wandb:      best_valid NDCG@100 0.1236
wandb:       best_valid NDCG@20 0.0532
wandb:       best_valid NDCG@50 0.0863
wandb:  best_valid Precision@10 0.0308
wandb: best_valid Precision@100 0.0328
wandb:  best_valid Precision@20 0.0331
wandb:  best_valid Precision@50 0.0341
wandb:     best_valid Recall@10 0.0385
wandb:    best_valid Recall@100 0.2814
wandb:     best_valid Recall@20 0.076
wandb:     best_valid Recall@50 0.1713
wandb:    epoch_loss_norm_train 86.19762
wandb:              test MRR@10 0.0895
wandb:             test MRR@100 0.1129
wandb:              test MRR@20 0.1019
wandb:              test MRR@50 0.1106
wandb:             test NDCG@10 0.0574
wandb:            test NDCG@100 0.1699
wandb:             test NDCG@20 0.0812
wandb:             test NDCG@50 0.1272
wandb:        test Precision@10 0.0359
wandb:       test Precision@100 0.0277
wandb:        test Precision@20 0.0357
wandb:        test Precision@50 0.0323
wandb:           test Recall@10 0.0772
wandb:          test Recall@100 0.4182
wandb:           test Recall@20 0.1398
wandb:           test Recall@50 0.2769
wandb:             valid MRR@10 0.0767
wandb:            valid MRR@100 0.1001
wandb:             valid MRR@20 0.0889
wandb:             valid MRR@50 0.0979
wandb:            valid NDCG@10 0.0395
wandb:           valid NDCG@100 0.1228
wandb:            valid NDCG@20 0.0531
wandb:            valid NDCG@50 0.0856
wandb:       valid Precision@10 0.0307
wandb:      valid Precision@100 0.0325
wandb:       valid Precision@20 0.0327
wandb:       valid Precision@50 0.0336
wandb:          valid Recall@10 0.0388
wandb:         valid Recall@100 0.2791
wandb:          valid Recall@20 0.0755
wandb:          valid Recall@50 0.1691
wandb: 
wandb: üöÄ View run T-DiffRec_ml-1m_noisy_1_V3 at: https://wandb.ai/drs/drs/runs/jiht9fvv
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230619_233428-jiht9fvv/logs
args: Namespace(batch_size=400, cuda=True, data_path='./datasets/', dataset='ml-1m_noisy', dims='[1000]', emb_size=10, epochs=1000, gpu='1', log_name='log', lr=0.0001, mean_type='x0', model_type='T-DiffRec', noise_max=0.02, noise_min=0.0001, noise_scale=0.1, noise_schedule='linear-var', norm=False, num_workers=0, reweight=True, round=1, run_name='V3', sampling_noise=False, sampling_steps=0, save_path='./saved_models/', seed=1, steps=5, time_type='cat', topN='[10, 20, 50, 100]', tst_w_val=False, w_max=1.0, w_min=0.1, weight_decay=0.0)
Starting time:  2023-06-19 23:34:31
user num: 5949
item num: 3494
data ready.
models ready.
Number of all parameters: 7002654
Start training...
Runing Epoch 001 train loss 5267.7868 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 002 train loss 4654.5959 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 003 train loss 4298.8084 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 004 train loss 4020.1168 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0354
0.0333
0.0309
0.0279
 0.0269
0.0476
0.1017
0.1653
 0.0407
0.0464
0.0642
0.0865
 0.0913
0.1007
0.1073
0.1091

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0302
0.0279
0.0238
0.0198
 0.0418
0.0747
0.1419
0.2189
 0.0412
0.0522
0.0762
0.1011
 0.0833
0.0925
0.0987
0.1004

Runing Epoch 005 train loss 3786.0733 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 006 train loss 3564.4580 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 007 train loss 3373.1069 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 008 train loss 3216.4874 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 009 train loss 3064.4935 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0345
0.0348
0.034
0.0315
 0.0289
0.0567
0.1251
0.206
 0.0402
0.0495
0.0735
0.1021
 0.0886
0.0995
0.1073
0.1091

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0336
0.0319
0.0282
0.0241
 0.0509
0.092
0.1836
0.2867
 0.0468
0.0618
0.0941
0.1272
 0.0896
0.1003
0.1073
0.1092

Runing Epoch 010 train loss 2937.8463 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 011 train loss 2815.4922 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 012 train loss 2716.2682 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 013 train loss 2624.8163 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 014 train loss 2533.3757 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.033
0.0346
0.0345
0.0327
 0.0323
0.0632
0.1394
0.2311
 0.0386
0.05
0.0771
0.1094
 0.0807
0.0924
0.1005
0.1025

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0345
0.0334
0.0304
0.0263
 0.059
0.1064
0.2155
0.3356
 0.0494
0.0674
0.1056
0.1434
 0.0878
0.0991
0.1068
0.1088

Runing Epoch 015 train loss 2449.9594 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 016 train loss 2380.3789 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 017 train loss 2322.4922 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 018 train loss 2260.4820 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 019 train loss 2200.4305 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0326
0.0342
0.0346
0.0331
 0.0342
0.0684
0.1501
0.2479
 0.0386
0.0512
0.0804
0.1147
 0.0784
0.0904
0.0989
0.1009

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0353
0.0342
0.0315
0.0271
 0.0654
0.1161
0.2375
0.3655
 0.0524
0.0718
0.1137
0.1536
 0.0899
0.1014
0.1096
0.1117

Runing Epoch 020 train loss 2163.1829 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 021 train loss 2109.3719 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 022 train loss 2069.4741 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 023 train loss 2027.7353 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 024 train loss 1989.0829 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.032
0.0334
0.0346
0.0332
 0.0357
0.0702
0.1567
0.2606
 0.0381
0.0509
0.0819
0.1177
 0.0755
0.0874
0.0962
0.0984

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0357
0.0346
0.0321
0.0277
 0.0696
0.1239
0.2503
0.3834
 0.0534
0.0741
0.1177
0.1589
 0.0866
0.0982
0.1066
0.1087

Runing Epoch 025 train loss 1954.8156 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 026 train loss 1924.7393 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 027 train loss 1895.1863 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 028 train loss 1867.8774 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 029 train loss 1838.4067 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0317
0.0336
0.0344
0.0335
 0.0376
0.0712
0.1596
0.2674
 0.0389
0.0518
0.0832
0.1203
 0.0756
0.0882
0.0968
0.099

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.036
0.0354
0.0325
0.0278
 0.0718
0.1279
0.2603
0.3949
 0.0552
0.0767
0.1217
0.163
 0.0893
0.1011
0.1095
0.1116

Runing Epoch 030 train loss 1813.7131 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 031 train loss 1793.7667 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 032 train loss 1768.4412 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 033 train loss 1746.6411 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 034 train loss 1727.0166 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0314
0.0333
0.0346
0.0332
 0.0382
0.0726
0.1621
0.2684
 0.0389
0.0521
0.0841
0.1208
 0.0754
0.0875
0.0964
0.0985

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0367
0.0357
0.0325
0.0278
 0.0754
0.1319
0.2645
0.3995
 0.0567
0.0784
0.1233
0.1649
 0.0904
0.1022
0.1106
0.1127

Runing Epoch 035 train loss 1710.8015 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 036 train loss 1690.6035 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 037 train loss 1676.0780 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 038 train loss 1659.5924 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 039 train loss 1644.2263 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0308
0.0324
0.0341
0.0331
 0.0369
0.0726
0.1647
0.2746
 0.038
0.0512
0.0839
0.1214
 0.0748
0.0868
0.096
0.0982

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0357
0.0359
0.0329
0.0282
 0.0743
0.1349
0.2719
0.4109
 0.0559
0.079
0.1253
0.1679
 0.089
0.1012
0.1099
0.112

Runing Epoch 040 train loss 1625.5738 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 041 train loss 1609.1979 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 042 train loss 1598.0066 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 043 train loss 1587.7579 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 044 train loss 1573.2999 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0307
0.0326
0.034
0.0329
 0.037
0.0754
0.1663
0.2752
 0.0385
0.0526
0.0848
0.1221
 0.0751
0.0876
0.0965
0.0987

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0365
0.0356
0.0328
0.0279
 0.077
0.1364
0.2726
0.412
 0.0587
0.0812
0.1274
0.1699
 0.0939
0.106
0.1146
0.1167

Runing Epoch 045 train loss 1557.2534 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 046 train loss 1546.6255 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 047 train loss 1538.7455 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 048 train loss 1522.8398 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 049 train loss 1511.3216 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.03
0.0325
0.034
0.0328
 0.0373
0.0735
0.1679
0.2786
 0.0382
0.0518
0.0851
0.1225
 0.0744
0.0872
0.0962
0.0984

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0363
0.0356
0.0327
0.028
 0.077
0.1369
0.2747
0.4166
 0.0567
0.0794
0.1258
0.1688
 0.088
0.1
0.1086
0.1107

Runing Epoch 050 train loss 1506.5156 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 051 train loss 1493.7192 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 052 train loss 1484.5436 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 053 train loss 1472.4604 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 054 train loss 1466.0944 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0306
0.0326
0.0339
0.0327
 0.0378
0.0734
0.1672
0.2778
 0.0387
0.0522
0.0851
0.1225
 0.0756
0.0877
0.0971
0.0992

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0365
0.0355
0.0326
0.028
 0.0777
0.1369
0.2761
0.4137
 0.0578
0.0803
0.1271
0.1693
 0.0902
0.1024
0.1109
0.113

Runing Epoch 055 train loss 1454.0570 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 056 train loss 1449.0493 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 057 train loss 1440.8881 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 058 train loss 1427.4991 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 059 train loss 1418.9947 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0308
0.0331
0.0341
0.0328
 0.0385
0.076
0.1713
0.2814
 0.0391
0.0532
0.0863
0.1236
 0.0762
0.0887
0.0979
0.1

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.037
0.0358
0.0328
0.028
 0.0781
0.139
0.2786
0.4193
 0.0586
0.0813
0.1283
0.171
 0.0919
0.104
0.1126
0.1148

Runing Epoch 060 train loss 1407.6691 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 061 train loss 1405.9620 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 062 train loss 1400.0174 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 063 train loss 1392.4348 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 064 train loss 1383.8948 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0305
0.0325
0.0336
0.0326
 0.0382
0.0738
0.1665
0.2787
 0.0396
0.0528
0.0852
0.1228
 0.0779
0.0903
0.0992
0.1014

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0366
0.0355
0.0323
0.0279
 0.0782
0.1386
0.2754
0.4177
 0.0579
0.0805
0.1265
0.1697
 0.0895
0.1015
0.1101
0.1123

Runing Epoch 065 train loss 1373.2175 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 066 train loss 1365.9686 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 067 train loss 1362.7889 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 068 train loss 1354.0290 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 069 train loss 1348.0794 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0303
0.0324
0.0336
0.0325
 0.0378
0.0738
0.1679
0.2773
 0.039
0.0524
0.0853
0.1223
 0.0766
0.0889
0.0981
0.1003

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0372
0.0357
0.0327
0.028
 0.0796
0.1402
0.2777
0.4168
 0.0586
0.0813
0.1277
0.1701
 0.09
0.1023
0.1109
0.1131

Runing Epoch 070 train loss 1343.6772 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 071 train loss 1336.3150 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 072 train loss 1332.5137 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 073 train loss 1323.3181 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 074 train loss 1319.1301 costs 00: 00: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0307
0.0327
0.0336
0.0325
 0.0388
0.0755
0.1691
0.2791
 0.0395
0.0531
0.0856
0.1228
 0.0767
0.0889
0.0979
0.1001

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0359
0.0357
0.0323
0.0277
 0.0772
0.1398
0.2769
0.4182
 0.0574
0.0812
0.1272
0.1699
 0.0895
0.1019
0.1106
0.1129

Runing Epoch 075 train loss 1310.0853 costs 00: 00: 03
------------------------------------------------------
Runing Epoch 076 train loss 1310.3202 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 077 train loss 1301.9089 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 078 train loss 1296.8624 costs 00: 00: 00
------------------------------------------------------
Runing Epoch 079 train loss 1292.9643 costs 00: 00: 00
------------------------------------------------------
------------------
Exiting from training early
======================================================
End. Best Epoch 060 
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0308
0.0331
0.0341
0.0328
 0.0385
0.076
0.1713
0.2814
 0.0391
0.0532
0.0863
0.1236
 0.0762
0.0887
0.0979
0.1

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.037
0.0358
0.0328
0.028
 0.0781
0.139
0.2786
0.4193
 0.0586
0.0813
0.1283
0.171
 0.0919
0.104
0.1126
0.1148

End time:  2023-06-19 23:36:07
wandb: Currently logged in as: piconda (drs). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/lcur2470/RS/wandb/run-20230619_233617-c9gds1l5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run T-DiffRec_amazon-book_clean_1_V3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/drs/drs
wandb: üöÄ View run at https://wandb.ai/drs/drs/runs/c9gds1l5
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    Epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÖ‚ñÖ‚ñÖ
wandb:         batch_loss_train ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ
wandb:         best_test MRR@10 ‚ñÅ
wandb:        best_test MRR@100 ‚ñÅ
wandb:         best_test MRR@20 ‚ñÅ
wandb:         best_test MRR@50 ‚ñÅ
wandb:        best_test NDCG@10 ‚ñÅ
wandb:       best_test NDCG@100 ‚ñÅ
wandb:        best_test NDCG@20 ‚ñÅ
wandb:        best_test NDCG@50 ‚ñÅ
wandb:   best_test Precision@10 ‚ñÅ
wandb:  best_test Precision@100 ‚ñÅ
wandb:   best_test Precision@20 ‚ñÅ
wandb:   best_test Precision@50 ‚ñÅ
wandb:      best_test Recall@10 ‚ñÅ
wandb:     best_test Recall@100 ‚ñÅ
wandb:      best_test Recall@20 ‚ñÅ
wandb:      best_test Recall@50 ‚ñÅ
wandb:        best_valid MRR@10 ‚ñÅ
wandb:       best_valid MRR@100 ‚ñÅ
wandb:        best_valid MRR@20 ‚ñÅ
wandb:        best_valid MRR@50 ‚ñÅ
wandb:       best_valid NDCG@10 ‚ñÅ
wandb:      best_valid NDCG@100 ‚ñÅ
wandb:       best_valid NDCG@20 ‚ñÅ
wandb:       best_valid NDCG@50 ‚ñÅ
wandb:  best_valid Precision@10 ‚ñÅ
wandb: best_valid Precision@100 ‚ñÅ
wandb:  best_valid Precision@20 ‚ñÅ
wandb:  best_valid Precision@50 ‚ñÅ
wandb:     best_valid Recall@10 ‚ñÅ
wandb:    best_valid Recall@100 ‚ñÅ
wandb:     best_valid Recall@20 ‚ñÅ
wandb:     best_valid Recall@50 ‚ñÅ
wandb:    epoch_loss_norm_train ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              test MRR@10 ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÅ‚ñà
wandb:             test MRR@100 ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:              test MRR@20 ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:              test MRR@50 ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:             test NDCG@10 ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:            test NDCG@100 ‚ñÉ‚ñá‚ñÑ‚ñà‚ñà‚ñÅ‚ñà
wandb:             test NDCG@20 ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:             test NDCG@50 ‚ñÉ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:        test Precision@10 ‚ñÉ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:       test Precision@100 ‚ñÖ‚ñá‚ñÑ‚ñà‚ñá‚ñÅ‚ñà
wandb:        test Precision@20 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:        test Precision@50 ‚ñÑ‚ñá‚ñÑ‚ñà‚ñá‚ñÅ‚ñà
wandb:           test Recall@10 ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:          test Recall@100 ‚ñÑ‚ñá‚ñÑ‚ñà‚ñá‚ñÅ‚ñà
wandb:           test Recall@20 ‚ñÉ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:           test Recall@50 ‚ñÑ‚ñá‚ñÑ‚ñà‚ñá‚ñÅ‚ñà
wandb:             valid MRR@10 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:            valid MRR@100 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:             valid MRR@20 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:             valid MRR@50 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:            valid NDCG@10 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:           valid NDCG@100 ‚ñÑ‚ñá‚ñÑ‚ñà‚ñá‚ñÅ‚ñà
wandb:            valid NDCG@20 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:            valid NDCG@50 ‚ñÑ‚ñá‚ñÑ‚ñà‚ñà‚ñÅ‚ñà
wandb:       valid Precision@10 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:      valid Precision@100 ‚ñÖ‚ñà‚ñÑ‚ñà‚ñá‚ñÅ‚ñà
wandb:       valid Precision@20 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:       valid Precision@50 ‚ñÖ‚ñá‚ñÑ‚ñà‚ñá‚ñÅ‚ñà
wandb:          valid Recall@10 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:         valid Recall@100 ‚ñÑ‚ñá‚ñÉ‚ñà‚ñá‚ñÅ‚ñà
wandb:          valid Recall@20 ‚ñÑ‚ñá‚ñÖ‚ñà‚ñà‚ñÅ‚ñà
wandb:          valid Recall@50 ‚ñÑ‚ñá‚ñÑ‚ñà‚ñá‚ñÅ‚ñà
wandb: 
wandb: Run summary:
wandb:                    Epoch 20
wandb:         batch_loss_train 0.63371
wandb:         best_test MRR@10 0.051
wandb:        best_test MRR@100 0.0571
wandb:         best_test MRR@20 0.054
wandb:         best_test MRR@50 0.0562
wandb:        best_test NDCG@10 0.0422
wandb:       best_test NDCG@100 0.0679
wandb:        best_test NDCG@20 0.0493
wandb:        best_test NDCG@50 0.0596
wandb:   best_test Precision@10 0.0137
wandb:  best_test Precision@100 0.0044
wandb:   best_test Precision@20 0.01
wandb:   best_test Precision@50 0.0063
wandb:      best_test Recall@10 0.06
wandb:     best_test Recall@100 0.1635
wandb:      best_test Recall@20 0.0835
wandb:      best_test Recall@50 0.1243
wandb:        best_valid MRR@10 0.0352
wandb:       best_valid MRR@100 0.0413
wandb:        best_valid MRR@20 0.0382
wandb:        best_valid MRR@50 0.0404
wandb:       best_valid NDCG@10 0.0234
wandb:      best_valid NDCG@100 0.0435
wandb:       best_valid NDCG@20 0.0286
wandb:       best_valid NDCG@50 0.0366
wandb:  best_valid Precision@10 0.011
wandb: best_valid Precision@100 0.0043
wandb:  best_valid Precision@20 0.0086
wandb:  best_valid Precision@50 0.0059
wandb:     best_valid Recall@10 0.0316
wandb:    best_valid Recall@100 0.104
wandb:     best_valid Recall@20 0.0474
wandb:     best_valid Recall@50 0.0757
wandb:    epoch_loss_norm_train 0.76055
wandb:              test MRR@10 0.0508
wandb:             test MRR@100 0.057
wandb:              test MRR@20 0.0538
wandb:              test MRR@50 0.0561
wandb:             test NDCG@10 0.0421
wandb:            test NDCG@100 0.068
wandb:             test NDCG@20 0.0492
wandb:             test NDCG@50 0.0596
wandb:        test Precision@10 0.0137
wandb:       test Precision@100 0.0044
wandb:        test Precision@20 0.01
wandb:        test Precision@50 0.0064
wandb:           test Recall@10 0.0602
wandb:          test Recall@100 0.1646
wandb:           test Recall@20 0.0839
wandb:           test Recall@50 0.125
wandb:             valid MRR@10 0.0348
wandb:            valid MRR@100 0.041
wandb:             valid MRR@20 0.0377
wandb:             valid MRR@50 0.0401
wandb:            valid NDCG@10 0.0232
wandb:           valid NDCG@100 0.0437
wandb:            valid NDCG@20 0.0284
wandb:            valid NDCG@50 0.0366
wandb:       valid Precision@10 0.011
wandb:      valid Precision@100 0.0043
wandb:       valid Precision@20 0.0086
wandb:       valid Precision@50 0.0059
wandb:          valid Recall@10 0.0315
wandb:         valid Recall@100 0.1055
wandb:          valid Recall@20 0.0474
wandb:          valid Recall@50 0.0762
wandb: 
wandb: üöÄ View run T-DiffRec_amazon-book_clean_1_V3 at: https://wandb.ai/drs/drs/runs/c9gds1l5
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230619_233617-c9gds1l5/logs
args: Namespace(batch_size=400, cuda=True, data_path='./datasets/', dataset='amazon-book_clean', dims='[1000]', emb_size=10, epochs=1000, gpu='1', log_name='log', lr=0.0001, mean_type='x0', model_type='T-DiffRec', noise_max=0.02, noise_min=0.0001, noise_scale=0.1, noise_schedule='linear-var', norm=False, num_workers=0, reweight=True, round=1, run_name='V3', sampling_noise=False, sampling_steps=0, save_path='./saved_models/', seed=1, steps=10, time_type='cat', topN='[10, 20, 50, 100]', tst_w_val=False, w_max=1.0, w_min=0.1, weight_decay=0.0)
Starting time:  2023-06-19 23:36:20
user num: 108822
item num: 94949
data ready.
models ready.
Number of all parameters: 190004159
Start training...
Runing Epoch 001 train loss 246.0661 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 002 train loss 224.9870 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 003 train loss 223.8875 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 004 train loss 221.4349 costs 00: 00: 37
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0073
0.0059
0.0042
0.0031
 0.019
0.0292
0.0493
0.0707
 0.0146
0.0179
0.0237
0.0289
 0.0236
0.0258
0.0275
0.0282

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0083
0.0063
0.0042
0.0031
 0.0337
0.0495
0.0781
0.1078
 0.0234
0.0282
0.0355
0.0418
 0.0293
0.0315
0.0332
0.0339

Runing Epoch 005 train loss 218.2920 costs 00: 03: 43
------------------------------------------------------
Runing Epoch 006 train loss 217.3379 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 007 train loss 215.7479 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 008 train loss 213.5661 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 009 train loss 213.1325 costs 00: 00: 38
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.01
0.008
0.0056
0.0042
 0.0275
0.0421
0.0701
0.0997
 0.0205
0.0253
0.0333
0.0404
 0.0315
0.0343
0.0366
0.0376

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0121
0.0091
0.006
0.0042
 0.0518
0.0738
0.114
0.1541
 0.0358
0.0424
0.0526
0.0611
 0.0436
0.0465
0.0487
0.0496

Runing Epoch 010 train loss 212.1619 costs 00: 03: 45
------------------------------------------------------
Runing Epoch 011 train loss 210.9387 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 012 train loss 210.4239 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 013 train loss 208.8459 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 014 train loss 209.3696 costs 00: 00: 38
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0081
0.0061
0.0039
0.0027
 0.0228
0.0327
0.0484
0.0627
 0.0175
0.0207
0.0253
0.0288
 0.0272
0.0291
0.0305
0.0309

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0105
0.0073
0.0043
0.0028
 0.0454
0.0602
0.0831
0.1033
 0.0328
0.0373
0.0432
0.0476
 0.0405
0.0424
0.0437
0.0442

Runing Epoch 015 train loss 209.0635 costs 00: 03: 40
------------------------------------------------------
Runing Epoch 016 train loss 208.8569 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 017 train loss 208.4416 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 018 train loss 208.4061 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 019 train loss 207.0349 costs 00: 00: 38
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.011
0.0086
0.0059
0.0043
 0.0316
0.0474
0.0757
0.104
 0.0234
0.0286
0.0366
0.0435
 0.0352
0.0382
0.0404
0.0413

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0137
0.01
0.0063
0.0044
 0.06
0.0835
0.1243
0.1635
 0.0422
0.0493
0.0596
0.0679
 0.051
0.054
0.0562
0.0571

Runing Epoch 020 train loss 208.8382 costs 00: 03: 47
------------------------------------------------------
Runing Epoch 021 train loss 207.7470 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 022 train loss 207.8144 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 023 train loss 207.4577 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 024 train loss 207.4017 costs 00: 00: 38
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0108
0.0084
0.0056
0.004
 0.0306
0.0454
0.0713
0.0967
 0.0229
0.0277
0.0351
0.0413
 0.0349
0.0377
0.0398
0.0406

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0134
0.0096
0.006
0.0041
 0.0588
0.0808
0.1176
0.1528
 0.0416
0.0483
0.0576
0.0651
 0.0505
0.0533
0.0554
0.0562

Runing Epoch 025 train loss 207.9674 costs 00: 03: 41
------------------------------------------------------
Runing Epoch 026 train loss 207.5667 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 027 train loss 207.2555 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 028 train loss 207.1887 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 029 train loss 207.1942 costs 00: 00: 38
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0043
0.0035
0.0024
0.0018
 0.0114
0.0177
0.0286
0.0405
 0.0091
0.0111
0.0142
0.0171
 0.0148
0.0161
0.0172
0.0176

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0055
0.0041
0.0026
0.0018
 0.0231
0.0325
0.0486
0.0651
 0.0167
0.0196
0.0237
0.0273
 0.0211
0.0224
0.0234
0.0238

Runing Epoch 030 train loss 206.5106 costs 00: 03: 40
------------------------------------------------------
Runing Epoch 031 train loss 207.6694 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 032 train loss 207.5514 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 033 train loss 207.2449 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 034 train loss 207.2535 costs 00: 00: 38
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.011
0.0086
0.0059
0.0043
 0.0315
0.0474
0.0762
0.1055
 0.0232
0.0284
0.0366
0.0437
 0.0348
0.0377
0.0401
0.041

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0137
0.01
0.0064
0.0044
 0.0602
0.0839
0.125
0.1646
 0.0421
0.0492
0.0596
0.068
 0.0508
0.0538
0.0561
0.057

Runing Epoch 035 train loss 207.7420 costs 00: 03: 41
------------------------------------------------------
Runing Epoch 036 train loss 207.2110 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 037 train loss 207.3652 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 038 train loss 207.4426 costs 00: 00: 37
------------------------------------------------------
Runing Epoch 039 train loss 207.6292 costs 00: 00: 38
------------------------------------------------------
------------------
Exiting from training early
======================================================
End. Best Epoch 020 
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.011
0.0086
0.0059
0.0043
 0.0316
0.0474
0.0757
0.104
 0.0234
0.0286
0.0366
0.0435
 0.0352
0.0382
0.0404
0.0413

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0137
0.01
0.0063
0.0044
 0.06
0.0835
0.1243
0.1635
 0.0422
0.0493
0.0596
0.0679
 0.051
0.054
0.0562
0.0571

End time:  2023-06-20 00:23:01
wandb: Currently logged in as: piconda (drs). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/lcur2470/RS/wandb/run-20230620_002314-nmieejis
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run T-DiffRec_amazon-book_noisy_1_V3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/drs/drs
wandb: üöÄ View run at https://wandb.ai/drs/drs/runs/nmieejis
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    Epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÖ‚ñÖ‚ñÖ
wandb:         batch_loss_train ‚ñá‚ñà‚ñÜ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÖ
wandb:         best_test MRR@10 ‚ñÅ
wandb:        best_test MRR@100 ‚ñÅ
wandb:         best_test MRR@20 ‚ñÅ
wandb:         best_test MRR@50 ‚ñÅ
wandb:        best_test NDCG@10 ‚ñÅ
wandb:       best_test NDCG@100 ‚ñÅ
wandb:        best_test NDCG@20 ‚ñÅ
wandb:        best_test NDCG@50 ‚ñÅ
wandb:   best_test Precision@10 ‚ñÅ
wandb:  best_test Precision@100 ‚ñÅ
wandb:   best_test Precision@20 ‚ñÅ
wandb:   best_test Precision@50 ‚ñÅ
wandb:      best_test Recall@10 ‚ñÅ
wandb:     best_test Recall@100 ‚ñÅ
wandb:      best_test Recall@20 ‚ñÅ
wandb:      best_test Recall@50 ‚ñÅ
wandb:        best_valid MRR@10 ‚ñÅ
wandb:       best_valid MRR@100 ‚ñÅ
wandb:        best_valid MRR@20 ‚ñÅ
wandb:        best_valid MRR@50 ‚ñÅ
wandb:       best_valid NDCG@10 ‚ñÅ
wandb:      best_valid NDCG@100 ‚ñÅ
wandb:       best_valid NDCG@20 ‚ñÅ
wandb:       best_valid NDCG@50 ‚ñÅ
wandb:  best_valid Precision@10 ‚ñÅ
wandb: best_valid Precision@100 ‚ñÅ
wandb:  best_valid Precision@20 ‚ñÅ
wandb:  best_valid Precision@50 ‚ñÅ
wandb:     best_valid Recall@10 ‚ñÅ
wandb:    best_valid Recall@100 ‚ñÅ
wandb:     best_valid Recall@20 ‚ñÅ
wandb:     best_valid Recall@50 ‚ñÅ
wandb:    epoch_loss_norm_train ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:              test MRR@10 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:             test MRR@100 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:              test MRR@20 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:              test MRR@50 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:             test NDCG@10 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:            test NDCG@100 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:             test NDCG@20 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:             test NDCG@50 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:        test Precision@10 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:       test Precision@100 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:        test Precision@20 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:        test Precision@50 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:           test Recall@10 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:          test Recall@100 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:           test Recall@20 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:           test Recall@50 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:             valid MRR@10 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:            valid MRR@100 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:             valid MRR@20 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:             valid MRR@50 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:            valid NDCG@10 ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÖ
wandb:           valid NDCG@100 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:            valid NDCG@20 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:            valid NDCG@50 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:       valid Precision@10 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÖ‚ñÜ
wandb:      valid Precision@100 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:       valid Precision@20 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:       valid Precision@50 ‚ñÑ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:          valid Recall@10 ‚ñÑ‚ñÇ‚ñÉ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÖ
wandb:         valid Recall@100 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:          valid Recall@20 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb:          valid Recall@50 ‚ñÉ‚ñÇ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÑ‚ñÜ
wandb: 
wandb: Run summary:
wandb:                    Epoch 25
wandb:         batch_loss_train 0.02261
wandb:         best_test MRR@10 0.0294
wandb:        best_test MRR@100 0.0332
wandb:         best_test MRR@20 0.0313
wandb:         best_test MRR@50 0.0327
wandb:        best_test NDCG@10 0.024
wandb:       best_test NDCG@100 0.0386
wandb:        best_test NDCG@20 0.0284
wandb:        best_test NDCG@50 0.034
wandb:   best_test Precision@10 0.0083
wandb:  best_test Precision@100 0.0026
wandb:   best_test Precision@20 0.0061
wandb:   best_test Precision@50 0.0038
wandb:      best_test Recall@10 0.0356
wandb:     best_test Recall@100 0.0931
wandb:      best_test Recall@20 0.0499
wandb:      best_test Recall@50 0.0716
wandb:        best_valid MRR@10 0.0193
wandb:       best_valid MRR@100 0.0228
wandb:        best_valid MRR@20 0.021
wandb:        best_valid MRR@50 0.0223
wandb:       best_valid NDCG@10 0.0131
wandb:      best_valid NDCG@100 0.023
wandb:       best_valid NDCG@20 0.0157
wandb:       best_valid NDCG@50 0.0197
wandb:  best_valid Precision@10 0.006
wandb: best_valid Precision@100 0.0023
wandb:  best_valid Precision@20 0.0047
wandb:  best_valid Precision@50 0.0032
wandb:     best_valid Recall@10 0.0185
wandb:    best_valid Recall@100 0.0559
wandb:     best_valid Recall@20 0.0273
wandb:     best_valid Recall@50 0.0418
wandb:    epoch_loss_norm_train 0.04019
wandb:              test MRR@10 0.0217
wandb:             test MRR@100 0.0245
wandb:              test MRR@20 0.023
wandb:              test MRR@50 0.0241
wandb:             test NDCG@10 0.0178
wandb:            test NDCG@100 0.0285
wandb:             test NDCG@20 0.0207
wandb:             test NDCG@50 0.025
wandb:        test Precision@10 0.0061
wandb:       test Precision@100 0.0019
wandb:        test Precision@20 0.0044
wandb:        test Precision@50 0.0028
wandb:           test Recall@10 0.026
wandb:          test Recall@100 0.0683
wandb:           test Recall@20 0.0358
wandb:           test Recall@50 0.0525
wandb:             valid MRR@10 0.013
wandb:            valid MRR@100 0.0157
wandb:             valid MRR@20 0.0143
wandb:             valid MRR@50 0.0153
wandb:            valid NDCG@10 0.0086
wandb:           valid NDCG@100 0.0159
wandb:            valid NDCG@20 0.0105
wandb:            valid NDCG@50 0.0135
wandb:       valid Precision@10 0.0041
wandb:      valid Precision@100 0.0017
wandb:       valid Precision@20 0.0033
wandb:       valid Precision@50 0.0023
wandb:          valid Recall@10 0.0121
wandb:         valid Recall@100 0.0393
wandb:          valid Recall@20 0.0184
wandb:          valid Recall@50 0.0294
wandb: 
wandb: üöÄ View run T-DiffRec_amazon-book_noisy_1_V3 at: https://wandb.ai/drs/drs/runs/nmieejis
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230620_002314-nmieejis/logs
args: Namespace(batch_size=400, cuda=True, data_path='./datasets/', dataset='amazon-book_noisy', dims='[1000]', emb_size=10, epochs=1000, gpu='1', log_name='log', lr=0.0001, mean_type='x0', model_type='T-DiffRec', noise_max=0.02, noise_min=0.0001, noise_scale=0.1, noise_schedule='linear-var', norm=False, num_workers=0, reweight=True, round=1, run_name='V3', sampling_noise=False, sampling_steps=0, save_path='./saved_models/', seed=1, steps=100, time_type='cat', topN='[10, 20, 50, 100]', tst_w_val=False, w_max=1.0, w_min=0.1, weight_decay=0.0)
Starting time:  2023-06-20 00:23:17
user num: 108822
item num: 178181
data ready.
models ready.
Number of all parameters: 356552291
Start training...
Runing Epoch 001 train loss 12.3743 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 002 train loss 12.0488 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 003 train loss 11.8799 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 004 train loss 11.7760 costs 00: 01: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0025
0.0019
0.0013
0.0009
 0.0072
0.0106
0.0161
0.0214
 0.0053
0.0063
0.0078
0.0091
 0.0085
0.0092
0.0098
0.01

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0032
0.0024
0.0015
0.001
 0.0135
0.0188
0.0272
0.035
 0.0092
0.0108
0.013
0.0147
 0.0117
0.0125
0.013
0.0132

Runing Epoch 005 train loss 11.4904 costs 00: 38: 35
------------------------------------------------------
Runing Epoch 006 train loss 11.5565 costs 00: 01: 01
------------------------------------------------------
Runing Epoch 007 train loss 11.3371 costs 00: 01: 01
------------------------------------------------------
Runing Epoch 008 train loss 11.2803 costs 00: 01: 01
------------------------------------------------------
Runing Epoch 009 train loss 11.2438 costs 00: 01: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.001
0.0008
0.0005
0.0003
 0.0027
0.0039
0.0057
0.0074
 0.0021
0.0024
0.0029
0.0033
 0.0035
0.0038
0.004
0.004

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0014
0.001
0.0006
0.0004
 0.0055
0.0074
0.0101
0.0127
 0.004
0.0045
0.0053
0.0058
 0.0052
0.0055
0.0056
0.0057

Runing Epoch 010 train loss 11.1836 costs 00: 38: 30
------------------------------------------------------
Runing Epoch 011 train loss 11.2123 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 012 train loss 11.1823 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 013 train loss 11.1373 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 014 train loss 11.2331 costs 00: 01: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0025
0.0021
0.0015
0.0011
 0.0071
0.0113
0.019
0.0263
 0.0051
0.0064
0.0084
0.0101
 0.0081
0.0089
0.0097
0.0099

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0036
0.0027
0.0018
0.0012
 0.0152
0.0216
0.0336
0.0444
 0.0102
0.0122
0.0152
0.0175
 0.0125
0.0134
0.0142
0.0145

Runing Epoch 015 train loss 11.2007 costs 00: 38: 34
------------------------------------------------------
Runing Epoch 016 train loss 11.0856 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 017 train loss 10.9575 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 018 train loss 10.9932 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 019 train loss 11.0226 costs 00: 01: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0052
0.0041
0.0028
0.002
 0.0161
0.0236
0.0368
0.0491
 0.0114
0.0137
0.0173
0.0202
 0.017
0.0185
0.0196
0.0201

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0073
0.0054
0.0035
0.0024
 0.0311
0.0441
0.065
0.0839
 0.0211
0.0251
0.0305
0.0346
 0.0258
0.0276
0.0289
0.0293

Runing Epoch 020 train loss 11.0337 costs 00: 38: 34
------------------------------------------------------
Runing Epoch 021 train loss 10.9916 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 022 train loss 10.9300 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 023 train loss 10.9208 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 024 train loss 10.9653 costs 00: 01: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.006
0.0047
0.0032
0.0023
 0.0185
0.0273
0.0418
0.0559
 0.0131
0.0157
0.0197
0.023
 0.0193
0.021
0.0223
0.0228

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0083
0.0061
0.0038
0.0026
 0.0356
0.0499
0.0716
0.0931
 0.024
0.0284
0.034
0.0386
 0.0294
0.0313
0.0327
0.0332

Runing Epoch 025 train loss 10.9808 costs 00: 38: 33
------------------------------------------------------
Runing Epoch 026 train loss 10.9683 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 027 train loss 10.9921 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 028 train loss 11.0349 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 029 train loss 11.0315 costs 00: 01: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0004
0.0003
0.0002
0.0001
 0.0009
0.0014
0.0021
0.0029
 0.0007
0.0009
0.0011
0.0012
 0.0014
0.0015
0.0016
0.0017

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0004
0.0003
0.0002
0.0001
 0.0015
0.0022
0.0033
0.0042
 0.0011
0.0012
0.0015
0.0017
 0.0014
0.0015
0.0016
0.0017

Runing Epoch 030 train loss 10.9347 costs 00: 38: 29
------------------------------------------------------
Runing Epoch 031 train loss 11.1098 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 032 train loss 10.9670 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 033 train loss 11.0172 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 034 train loss 10.9940 costs 00: 01: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0033
0.0025
0.0016
0.0011
 0.0096
0.0135
0.0197
0.0257
 0.0071
0.0082
0.0099
0.0114
 0.011
0.0118
0.0124
0.0126

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0048
0.0034
0.0021
0.0014
 0.0204
0.0272
0.0372
0.0461
 0.0145
0.0166
0.0193
0.0213
 0.0182
0.0192
0.0198
0.0201

Runing Epoch 035 train loss 10.9445 costs 00: 38: 28
------------------------------------------------------
Runing Epoch 036 train loss 11.1241 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 037 train loss 10.9671 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 038 train loss 10.9905 costs 00: 00: 59
------------------------------------------------------
Runing Epoch 039 train loss 10.9869 costs 00: 01: 00
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0041
0.0033
0.0023
0.0017
 0.0121
0.0184
0.0294
0.0393
 0.0086
0.0105
0.0135
0.0159
 0.013
0.0143
0.0153
0.0157

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0061
0.0044
0.0028
0.0019
 0.026
0.0358
0.0525
0.0683
 0.0178
0.0207
0.025
0.0285
 0.0217
0.023
0.0241
0.0245

Runing Epoch 040 train loss 10.9525 costs 00: 38: 28
------------------------------------------------------
Runing Epoch 041 train loss 11.1053 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 042 train loss 10.9293 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 043 train loss 11.0195 costs 00: 01: 00
------------------------------------------------------
Runing Epoch 044 train loss 10.9710 costs 00: 01: 00
------------------------------------------------------
------------------
Exiting from training early
======================================================
End. Best Epoch 025 
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.006
0.0047
0.0032
0.0023
 0.0185
0.0273
0.0418
0.0559
 0.0131
0.0157
0.0197
0.023
 0.0193
0.021
0.0223
0.0228

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0083
0.0061
0.0038
0.0026
 0.0356
0.0499
0.0716
0.0931
 0.024
0.0284
0.034
0.0386
 0.0294
0.0313
0.0327
0.0332

End time:  2023-06-20 06:08:08
