Matplotlib is building the font cache; this may take a moment.
wandb: Currently logged in as: piconda (drs). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in /home/lcur2420/RS/wandb/run-20230622_183159-3sqx9b1z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run T-DiffRec_amazon-book_clean_1_LUKE_V2
wandb:  View project at https://wandb.ai/drs/drs
wandb:  View run at https://wandb.ai/drs/drs/runs/3sqx9b1z
args: Namespace(batch_size=400, cuda=True, data_path='./datasets/', dataset='amazon-book_clean', dims='[1000]', emb_size=10, epochs=1000, gpu='1', log_name='log', lr=0.0001, mean_type='x0_learnable', model_type='T-DiffRec', noise_max=0.02, noise_min=0.0001, noise_scale=0.1, noise_schedule='linear-var', norm=False, num_workers=0, patience=20, reweight=True, round=1, run_name='LUKE_V2', sampling_noise=False, sampling_steps=0, save_path='./saved_models/', seed=1, steps=10, time_type='cat', topN='[10, 20, 50, 100]', tst_w_val=False, visualize_weights=True, w_max=1.0, w_min=0.1, weight_decay=0.0)
Starting time:  2023-06-22 18:32:02
user num: 108822
item num: 94949
data ready.
Embedding size: 10
models ready.
Number of all parameters: 190004159
Start training...
Runing Epoch 001 train loss 246.0661 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 002 train loss 224.9870 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 003 train loss 223.8875 costs 00: 00: 41
------------------------------------------------------
Runing Epoch 004 train loss 221.4349 costs 00: 00: 42
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0073
0.0059
0.0042
0.0031
 0.019
0.0292
0.0493
0.0707
 0.0146
0.0179
0.0237
0.0289
 0.0236
0.0258
0.0275
0.0282

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0083
0.0063
0.0042
0.0031
 0.0337
0.0495
0.0781
0.1078
 0.0234
0.0282
0.0355
0.0418
 0.0293
0.0315
0.0332
0.0339

Runing Epoch 005 train loss 218.2920 costs 00: 03: 52
------------------------------------------------------
Runing Epoch 006 train loss 217.3379 costs 00: 00: 40
------------------------------------------------------
Runing Epoch 007 train loss 215.7479 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 008 train loss 213.5661 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 009 train loss 213.1325 costs 00: 00: 42
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.01
0.008
0.0056
0.0042
 0.0275
0.0421
0.0701
0.0997
 0.0205
0.0253
0.0333
0.0404
 0.0315
0.0343
0.0366
0.0376

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0121
0.0091
0.006
0.0042
 0.0518
0.0738
0.114
0.1541
 0.0358
0.0424
0.0526
0.0611
 0.0436
0.0465
0.0487
0.0496

Runing Epoch 010 train loss 212.1619 costs 00: 03: 50
------------------------------------------------------
Runing Epoch 011 train loss 210.9387 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 012 train loss 210.4239 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 013 train loss 208.8459 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 014 train loss 209.3696 costs 00: 00: 38
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0081
0.0061
0.0039
0.0027
 0.0228
0.0327
0.0484
0.0627
 0.0175
0.0207
0.0253
0.0288
 0.0272
0.0291
0.0305
0.0309

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0105
0.0073
0.0043
0.0028
 0.0454
0.0602
0.0831
0.1033
 0.0328
0.0373
0.0432
0.0476
 0.0405
0.0424
0.0437
0.0442

Runing Epoch 015 train loss 209.0635 costs 00: 03: 44
------------------------------------------------------
Runing Epoch 016 train loss 208.8569 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 017 train loss 208.4416 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 018 train loss 208.4061 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 019 train loss 207.0349 costs 00: 00: 39
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.011
0.0086
0.0059
0.0043
 0.0316
0.0474
0.0757
0.104
 0.0234
0.0286
0.0366
0.0435
 0.0352
0.0382
0.0404
0.0413

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0137
0.01
0.0063
0.0044
 0.06
0.0835
0.1243
0.1635
 0.0422
0.0493
0.0596
0.0679
 0.051
0.054
0.0562
0.0571

Runing Epoch 020 train loss 208.8382 costs 00: 03: 45
------------------------------------------------------
Runing Epoch 021 train loss 207.7470 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 022 train loss 207.8144 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 023 train loss 207.4577 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 024 train loss 207.4017 costs 00: 00: 42
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0108
0.0084
0.0056
0.004
 0.0306
0.0454
0.0713
0.0967
 0.0229
0.0277
0.0351
0.0413
 0.0349
0.0377
0.0398
0.0406

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0134
0.0096
0.006
0.0041
 0.0588
0.0808
0.1176
0.1528
 0.0416
0.0483
0.0576
0.0651
 0.0505
0.0533
0.0554
0.0562

Runing Epoch 025 train loss 207.9674 costs 00: 03: 48
------------------------------------------------------
Runing Epoch 026 train loss 207.5667 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 027 train loss 207.2555 costs 00: 00: 40
------------------------------------------------------
Runing Epoch 028 train loss 207.1887 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 029 train loss 207.1942 costs 00: 00: 42
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0043
0.0035
0.0024
0.0018
 0.0114
0.0177
0.0286
0.0405
 0.0091
0.0111
0.0142
0.0171
 0.0148
0.0161
0.0172
0.0176

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0055
0.0041
0.0026
0.0018
 0.0231
0.0325
0.0486
0.0651
 0.0167
0.0196
0.0237
0.0273
 0.0211
0.0224
0.0234
0.0238

Runing Epoch 030 train loss 206.5106 costs 00: 03: 50
------------------------------------------------------
Runing Epoch 031 train loss 207.6694 costs 00: 00: 43
------------------------------------------------------
Runing Epoch 032 train loss 207.5514 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 033 train loss 207.2449 costs 00: 00: 42
------------------------------------------------------
Runing Epoch 034 train loss 207.2535 costs 00: 00: 42
------------------------------------------------------
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.011
0.0086
0.0059
0.0043
 0.0315
0.0474
0.0762
0.1055
 0.0232
0.0284
0.0366
0.0437
 0.0348
0.0377
0.0401
0.041

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0137
0.01
0.0064
0.0044
 0.0602
0.0839
0.125
0.1646
 0.0421
0.0492
0.0596
0.068
 0.0508
0.0538
0.0561
0.057

Runing Epoch 035 train loss 207.7420 costs 00: 03: 49
------------------------------------------------------
Runing Epoch 036 train loss 207.2110 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 037 train loss 207.3652 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 038 train loss 207.4426 costs 00: 00: 38
------------------------------------------------------
Runing Epoch 039 train loss 207.6292 costs 00: 00: 38
------------------------------------------------------
------------------
Exiting from training early
======================================================
End. Best Epoch 020 
[Valid]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.011
0.0086
0.0059
0.0043
 0.0316
0.0474
0.0757
0.104
 0.0234
0.0286
0.0366
0.0435
 0.0352
0.0382
0.0404
0.0413

[Test]: Precisions (4), Recalls (4), NDCGs (4), MRRs (4):

0.0137
0.01
0.0063
0.0044
 0.06
0.0835
0.1243
0.1635
 0.0422
0.0493
0.0596
0.0679
 0.051
0.054
0.0562
0.0571

End time:  2023-06-22 19:21:01
params_per_batch.shape: torch.Size([10646, 10, 10])
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run summary:
wandb:                    Epoch 20
wandb:         batch_loss_train 0.63371
wandb:         best_test MRR@10 0.051
wandb:        best_test MRR@100 0.0571
wandb:         best_test MRR@20 0.054
wandb:         best_test MRR@50 0.0562
wandb:        best_test NDCG@10 0.0422
wandb:       best_test NDCG@100 0.0679
wandb:        best_test NDCG@20 0.0493
wandb:        best_test NDCG@50 0.0596
wandb:   best_test Precision@10 0.0137
wandb:  best_test Precision@100 0.0044
wandb:   best_test Precision@20 0.01
wandb:   best_test Precision@50 0.0063
wandb:      best_test Recall@10 0.06
wandb:     best_test Recall@100 0.1635
wandb:      best_test Recall@20 0.0835
wandb:      best_test Recall@50 0.1243
wandb:        best_valid MRR@10 0.0352
wandb:       best_valid MRR@100 0.0413
wandb:        best_valid MRR@20 0.0382
wandb:        best_valid MRR@50 0.0404
wandb:       best_valid NDCG@10 0.0234
wandb:      best_valid NDCG@100 0.0435
wandb:       best_valid NDCG@20 0.0286
wandb:       best_valid NDCG@50 0.0366
wandb:  best_valid Precision@10 0.011
wandb: best_valid Precision@100 0.0043
wandb:  best_valid Precision@20 0.0086
wandb:  best_valid Precision@50 0.0059
wandb:     best_valid Recall@10 0.0316
wandb:    best_valid Recall@100 0.104
wandb:     best_valid Recall@20 0.0474
wandb:     best_valid Recall@50 0.0757
wandb:    epoch_loss_norm_train 0.76055
wandb:              test MRR@10 0.0508
wandb:             test MRR@100 0.057
wandb:              test MRR@20 0.0538
wandb:              test MRR@50 0.0561
wandb:             test NDCG@10 0.0421
wandb:            test NDCG@100 0.068
wandb:             test NDCG@20 0.0492
wandb:             test NDCG@50 0.0596
wandb:        test Precision@10 0.0137
wandb:       test Precision@100 0.0044
wandb:        test Precision@20 0.01
wandb:        test Precision@50 0.0064
wandb:           test Recall@10 0.0602
wandb:          test Recall@100 0.1646
wandb:           test Recall@20 0.0839
wandb:           test Recall@50 0.125
wandb:             valid MRR@10 0.0348
wandb:            valid MRR@100 0.041
wandb:             valid MRR@20 0.0377
wandb:             valid MRR@50 0.0401
wandb:            valid NDCG@10 0.0232
wandb:           valid NDCG@100 0.0437
wandb:            valid NDCG@20 0.0284
wandb:            valid NDCG@50 0.0366
wandb:       valid Precision@10 0.011
wandb:      valid Precision@100 0.0043
wandb:       valid Precision@20 0.0086
wandb:       valid Precision@50 0.0059
wandb:          valid Recall@10 0.0315
wandb:         valid Recall@100 0.1055
wandb:          valid Recall@20 0.0474
wandb:          valid Recall@50 0.0762
wandb: 
wandb:  View run T-DiffRec_amazon-book_clean_1_LUKE_V2 at: https://wandb.ai/drs/drs/runs/3sqx9b1z
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230622_183159-3sqx9b1z/logs
Traceback (most recent call last):
  File "./T-DiffRec/main.py", line 431, in <module>
    np.save(
  File "<__array_function__ internals>", line 180, in save
  File "/home/lcur2420/.conda/envs/rs/lib/python3.8/site-packages/numpy/lib/npyio.py", line 515, in save
    file_ctx = open(file, "wb")
FileNotFoundError: [Errno 2] No such file or directory: 'mPHATE/T-DiffRec_amazon-book_clean_1_LUKE_V2_params_per_batch.npy'
Traceback (most recent call last):
  File "utils/visualize_weights.py", line 64, in <module>
    main()
  File "utils/visualize_weights.py", line 51, in main
    data = np.load(
  File "/home/lcur2420/.conda/envs/rs/lib/python3.8/site-packages/numpy/lib/npyio.py", line 407, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'mPHATE/T-DiffRec_amazon-book_clean_1_LUKE_V2_params_per_batch.npy'
